{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q SentencePiece transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Generator Definitions\n",
    "\n",
    "**`generate_aa`**\n",
    "- *Protein Amino Acids* **:** https://www.cryst.bbk.ac.uk/education/AminoAcid/the_twenty.html\n",
    "\n",
    "**`generate_pcp` & `generate_psp`**\n",
    "- *`inference_utils.py`* **:** https://github.com/thuxugang/opus_tass/blob/master/inference_utils.py\n",
    "\n",
    "**`generate_prottrans`**\n",
    "- *ProtTrans* **:** https://github.com/agemagician/ProtTrans\n",
    "- *`ProtT5-XL-UniRef50.ipynb`* **:** https://github.com/agemagician/ProtTrans/blob/master/Embedding/PyTorch/Advanced/ProtT5-XL-UniRef50.ipynb\n",
    "- *`generate_prottrans.py`* **:** https://github.com/jas-preet/SPOT-1D-LM/blob/main/generate_prottrans.py\n",
    "\n",
    "**`generate_esm`**\n",
    "- *Meta Fundamental AI Research (FAIR) Evolutionary Scale Modeling (ESM)* **:** https://github.com/facebookresearch/esm\n",
    "- *`generate_esm.py`* **:** https://github.com/jas-preet/SPOT-1D-LM/blob/main/generate_esm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pssm(pssm_file_path, pseq, output_file_path):\n",
    "    num_pssm_columns = 44\n",
    "    pssm_column_names = [str(index) for index in range(num_pssm_columns)]\n",
    "    \n",
    "    with open(pssm_file_path, 'r') as pssm_file:\n",
    "        pssm = pd.read_csv(pssm_file, names=pssm_column_names, delim_whitespace=True).dropna().values[:, 2:22].astype(np.float32)\n",
    "    \n",
    "    assert pssm.shape[0] == len(pseq), \"PSSM file is in wrong format!\"\n",
    "    \n",
    "    pssm = (pssm - np.mean(pssm, axis=0, keepdims=True)) / np.std(pssm, axis=0, keepdims=True)\n",
    "    \n",
    "    with open(output_file_path, 'wb') as pssm_file:\n",
    "        np.save(file=pssm_file, arr=pssm)\n",
    "    \n",
    "    return pssm\n",
    "\n",
    "def generate_hhm(hhm_file_path, pseq, output_file_path):\n",
    "    num_hhm_columns = 22\n",
    "    hhm_column_names = [str(index) for index in range(num_hhm_columns)]\n",
    "    \n",
    "    with open(hhm_file_path, 'r') as hhm_file:\n",
    "        hhm = pd.read_csv(hhm_file, names=hhm_column_names, delim_whitespace=True)\n",
    "    \n",
    "    pos1 = (hhm[\"0\"] == \"HMM\").idxmax() + 3\n",
    "    num_columns = len(hhm.columns)\n",
    "    hhm = hhm[pos1:-1].values[:, :num_hhm_columns].reshape((-1, 44))\n",
    "    hhm[hhm == '*'] = \"9999\"\n",
    "    hhm = hhm[:, 2:-12].astype(np.float32)\n",
    "    \n",
    "    assert hhm.shape[0] == len(pseq), \"HHM file is in wrong format!\"\n",
    "    \n",
    "    hhm = (hhm - np.mean(hhm, axis=0, keepdims=True)) / np.std(hhm, axis=0, keepdims=True)\n",
    "    \n",
    "    with open(output_file_path, 'wb') as hhm_file:\n",
    "        np.save(file=hhm_file, arr=hhm)\n",
    "    \n",
    "    return hhm\n",
    "\n",
    "def get_pcp_dictionary():\n",
    "    pcp_dictionary = {\n",
    "        'A': [-0.350, -0.680, -0.677, -0.171, -0.170, 0.900, -0.476], \n",
    "        'R': [0.105, 0.373, 0.466, -0.900, 0.900, 0.528, -0.371], \n",
    "        'N': [-0.213, -0.329, -0.243, -0.674, -0.075, -0.403, -0.529], \n",
    "        'D': [-0.213, -0.417, -0.281, -0.767, -0.900, -0.155, -0.635], \n",
    "        'C': [-0.140, -0.329, -0.359, 0.508, -0.114, -0.652, 0.476], \n",
    "        'Q': [-0.230, -0.110, -0.020, -0.464, -0.276, 0.528, -0.371], \n",
    "        'E': [-0.230, -0.241, -0.058, -0.696, -0.868, 0.900, -0.582], \n",
    "        'G': [-0.900, -0.900, -0.900, -0.342, -0.179, -0.900, -0.900], \n",
    "        'H': [0.384, 0.110, 0.138, -0.271, 0.195, -0.031, -0.106], \n",
    "        'I': [0.900, -0.066, -0.009, 0.652, -0.186, 0.155, 0.688], \n",
    "        'L': [0.213, -0.066, -0.009, 0.596, -0.186, 0.714, -0.053], \n",
    "        'K': [-0.088, 0.066, 0.163, -0.889, 0.727, 0.279, -0.265], \n",
    "        'M': [0.110, 0.066, 0.087, 0.337, -0.262, 0.652, -0.001], \n",
    "        'F': [0.363, 0.373, 0.412, 0.646, -0.272, 0.155, 0.318], \n",
    "        'P': [0.247, -0.900, -0.294, 0.055, -0.010, -0.900, 0.106], \n",
    "        'S': [-0.337, -0.637, -0.544, -0.364, -0.265, -0.466, -0.212], \n",
    "        'T': [0.402, -0.417, -0.321, -0.199, -0.288, -0.403, 0.212], \n",
    "        'W': [0.479, 0.900, 0.900, 0.900, -0.209, 0.279, 0.529], \n",
    "        'Y': [0.363, 0.417, 0.541, 0.188, -0.274, -0.155, 0.476], \n",
    "        'V': [0.677, -0.285, -0.232, 0.331, -0.191, -0.031, 0.900]\n",
    "    }\n",
    "    return pcp_dictionary\n",
    "\n",
    "def generate_pcp(pseq, output_file_path):\n",
    "    pcp_dictionary = get_pcp_dictionary()\n",
    "    pcp = np.array([pcp_dictionary.get(amino_acid_residue, [0] * 7) for amino_acid_residue in pseq], dtype=np.float32)\n",
    "    \n",
    "    pcp = (pcp - np.mean(pcp, axis=0, keepdims=True)) / np.std(pcp, axis=0, keepdims=True)\n",
    "    \n",
    "    with open(output_file_path, 'wb') as pcp_file:\n",
    "        np.save(file=pcp_file, arr=pcp)\n",
    "    \n",
    "    return pcp\n",
    "\n",
    "def generate_aa(pseq, output_file_path):\n",
    "    amino_acid_residues = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    aa = np.zeros(shape=(len(pseq), 20), dtype=np.float32)\n",
    "    \n",
    "    for index, amino_acid_residue in enumerate(pseq):\n",
    "        if amino_acid_residue in amino_acid_residues:\n",
    "            aa[index, amino_acid_residues.index(amino_acid_residue)] = 1\n",
    "    \n",
    "    with open(output_file_path, 'wb') as aa_file:\n",
    "        np.save(file=aa_file, arr=aa)\n",
    "    \n",
    "    return aa\n",
    "\n",
    "def get_psp_dictionary():\n",
    "    psp_dictionary = {\n",
    "        'A': [1, 3, 7], \n",
    "        'R': [1, 5, 6, 7, 13], \n",
    "        'N': [1, 5, 7, 14], \n",
    "        'D': [1, 5, 7, 11], \n",
    "        'C': [1, 7, 8], \n",
    "        'Q': [1, 6, 7, 14], \n",
    "        'E': [1, 6, 7, 11], \n",
    "        'G': [1, 4, 7], \n",
    "        'H': [1, 5, 7, 17], \n",
    "        'I': [1, 3, 7, 12], \n",
    "        'L': [1, 5, 7, 12], \n",
    "        'K': [1, 5, 6, 7, 10], \n",
    "        'M': [1, 6, 7, 9], \n",
    "        'F': [1, 5, 7, 16], \n",
    "        'P': [7, 19], \n",
    "        'S': [1, 2, 5, 7], \n",
    "        'T': [1, 7, 15], \n",
    "        'W': [1, 5, 7, 18], \n",
    "        'Y': [1, 2, 5, 7, 16], \n",
    "        'V': [1, 7, 12]\n",
    "    }\n",
    "    return psp_dictionary\n",
    "\n",
    "def generate_psp(pseq, output_file_path):\n",
    "    psp_dictionary = get_psp_dictionary()\n",
    "    psp = np.zeros(shape=(len(pseq), 19), dtype=np.float32)\n",
    "    \n",
    "    for index, amino_acid_residue in enumerate(pseq):\n",
    "        if amino_acid_residue in psp_dictionary:\n",
    "            indices = psp_dictionary[amino_acid_residue]\n",
    "            \n",
    "            for idx in indices:\n",
    "                psp[index, idx - 1] = 1\n",
    "    \n",
    "    with open(output_file_path, 'wb') as psp_file:\n",
    "        np.save(file=psp_file, arr=psp)\n",
    "    \n",
    "    return psp\n",
    "\n",
    "def generate_contact(contact_file_path, pseq, output_file_path, window_size, min_sep=3):\n",
    "    if not window_size > 0:\n",
    "        return None\n",
    "    \n",
    "    pseq_length = len(pseq)\n",
    "    contact_feats = np.zeros(shape=(pseq_length, pseq_length, 1))\n",
    "    \n",
    "    with open(contact_file_path, 'r') as contact_file:\n",
    "        contact_map = pd.read_csv(contact_file, names=[\"pos1\", \"pos2\", \"idk1\", \"idk2\", \"score\"], delim_whitespace=True)\n",
    "    \n",
    "    contact_map = contact_map[contact_map[\"pos1\"].astype(str).str.isdigit()].dropna().values\n",
    "    \n",
    "    if contact_map.shape[0] == 0:\n",
    "        with open(contact_file_path, 'r') as contact_file:\n",
    "            contact_map = pd.read_csv(contact_file, names=[\"pos1\", \"pos2\", \"score\"], delim_whitespace=True)\n",
    "        \n",
    "        contact_map = contact_map[contact_map[\"pos1\"].astype(str).str.isdigit()].dropna().values\n",
    "        pos1 = contact_map[:, 0].astype(int)\n",
    "        pos2 = contact_map[:, 1].astype(int)\n",
    "    else:\n",
    "        pos1 = contact_map[:, 0].astype(int) - 1\n",
    "        pos2 = contact_map[:, 1].astype(int) - 1\n",
    "    \n",
    "    score = contact_map[:, -1:]\n",
    "    contact_feats[pos1, pos2] = score\n",
    "    contact_feats = contact_feats + np.transpose(contact_feats, axes=(1, 0, 2)) + np.tril(m=np.triu(m=np.ones(shape=(pseq_length, pseq_length)), k=(-min_sep + 1)), k=(min_sep - 1))[:, :, None]\n",
    "    \n",
    "    contact_image = []\n",
    "    contact_image.append(contact_feats)\n",
    "    contact_image = np.concatenate(contact_image, axis=2)\n",
    "    \n",
    "    assert pseq_length == contact_image.shape[0]\n",
    "    \n",
    "    features_depth = contact_image.shape[2]\n",
    "    window_size = int(window_size)\n",
    "    \n",
    "    resize = np.concatenate([np.zeros(shape=(window_size, pseq_length, features_depth)), np.concatenate([contact_image, np.zeros(shape=(window_size, pseq_length, features_depth))], axis=0)], axis=0)\n",
    "    contact_array = np.concatenate([resize[index:(index + 2 * window_size + 1), index, :features_depth] for index in range(pseq_length)], axis=1).T\n",
    "    removal_indices = np.array([window_size + index for index in range(-2, 3)])\n",
    "    contact = np.delete(contact_array, obj=removal_indices, axis=1).astype(np.float32)\n",
    "    \n",
    "    contact = (contact - np.mean(contact, axis=0, keepdims=True)) / np.std(contact, axis=0, keepdims=True)\n",
    "    \n",
    "    with open(output_file_path, 'wb') as contact_file:\n",
    "        np.save(file=contact_file, arr=contact)\n",
    "    \n",
    "    return contact\n",
    "\n",
    "def generate_prottrans(pseq, output_file_path, use_gpu=False):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    gc.collect()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\") if use_gpu and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    \n",
    "    p_s_e_q = [re.sub(r\"[UZOB]\", 'X', ' '.join(pseq))]\n",
    "    \n",
    "    ids = tokenizer.batch_encode_plus(p_s_e_q, add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(ids[\"attention_mask\"]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "    prottrans = []\n",
    "    \n",
    "    for sequence_num in range(len(embedding)):\n",
    "        sequence_len = (attention_mask[sequence_num] == 1).sum()\n",
    "        sequence_emd = embedding[sequence_num][:sequence_len - 1]\n",
    "        prottrans.append(sequence_emd)\n",
    "    \n",
    "    with open(output_file_path, 'wb') as prottrans_file:\n",
    "        np.save(file=prottrans_file, arr=prottrans[0])\n",
    "    \n",
    "    return prottrans[0]\n",
    "\n",
    "def generate_esm(protein_name, pseq, output_file_path, use_gpu=False):\n",
    "    model, alphabet = torch.hub.load(repo_or_dir=\"facebookresearch/esm\", model=\"esm1b_t33_650M_UR50S\", verbose=False)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\") if use_gpu and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    \n",
    "    data = [(protein_name, pseq)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    \n",
    "    token_representations = results[\"representations\"][33]\n",
    "    \n",
    "    for index, (name, sequence) in enumerate(data):\n",
    "        esm = token_representations[index, 1:len(sequence) + 1].cpu().numpy()\n",
    "        \n",
    "        with open(output_file_path, 'wb') as esm_file:\n",
    "            np.save(file=esm_file, arr=esm)\n",
    "    \n",
    "    return esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_prottrans(proteins_dict, use_gpu=False):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    gc.collect()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\") if use_gpu and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    \n",
    "    for protein_name in tqdm(iterable=proteins_dict, desc=f\"Generating ProtTrans Feature...\", ncols=100, unit=\"protein\"):\n",
    "        p_s_e_q = [re.sub(r\"[UZOB]\", 'X', ' '.join(proteins_dict[protein_name][\"pseq\"]))]\n",
    "        \n",
    "        ids = tokenizer.batch_encode_plus(p_s_e_q, add_special_tokens=True, padding=True)\n",
    "        input_ids = torch.tensor(ids[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(ids[\"attention_mask\"]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "        prottrans = []\n",
    "        \n",
    "        for sequence_num in range(len(embedding)):\n",
    "            sequence_len = (attention_mask[sequence_num] == 1).sum()\n",
    "            sequence_emd = embedding[sequence_num][:sequence_len - 1]\n",
    "            prottrans.append(sequence_emd)\n",
    "        \n",
    "        with open(proteins_dict[protein_name][\"output_file_path\"] + \"_prottrans.npy\", 'wb') as prottrans_file:\n",
    "            np.save(file=prottrans_file, arr=prottrans[0])\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_batch_esm(proteins_dict, use_gpu=False):\n",
    "    model, alphabet = torch.hub.load(repo_or_dir=\"facebookresearch/esm\", model=\"esm1b_t33_650M_UR50S\", verbose=False)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\") if use_gpu and torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    \n",
    "    for protein_name in tqdm(iterable=proteins_dict, desc=f\"Generating ESM-1b Feature...\", ncols=100, unit=\"protein\"):\n",
    "        data = [(protein_name, proteins_dict[protein_name][\"pseq\"])]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        \n",
    "        token_representations = results[\"representations\"][33]\n",
    "        \n",
    "        for index, (name, sequence) in enumerate(data):\n",
    "            esm = token_representations[index, 1:len(sequence) + 1].cpu().numpy()\n",
    "            \n",
    "            with open(proteins_dict[protein_name][\"output_file_path\"] + \"_esm.npy\", 'wb') as esm_file:\n",
    "                np.save(file=esm_file, arr=esm)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels Generator Definitions\n",
    "\n",
    "**`generate_ss3_ss8_phi_psi`**\n",
    "- *SPOT-1D-Single Paper* **:** https://doi.org/10.1093/bioinformatics/btab316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ss3_ss8_phi_psi(protein_name, dssp_file_path, output_dir_path):\n",
    "    with open(dssp_file_path, 'r') as dssp_file:\n",
    "        contents = [content for content in dssp_file.read().split('\\n') if content != '']\n",
    "    \n",
    "    secondary_structures = contents[2]\n",
    "    phi_angles = [(-500 if phi == 'X' else float(phi)) for phi in contents[3].split() if phi != '']\n",
    "    psi_angles = [(-500 if psi == 'X' else float(psi)) for psi in contents[4].split() if psi != '']\n",
    "    \n",
    "    assert len(contents[1]) == len(secondary_structures) == len(phi_angles) == len(psi_angles)\n",
    "    \n",
    "    ss3 = np.zeros(shape=(len(secondary_structures), 3))\n",
    "    ss8 = np.zeros(shape=(len(secondary_structures), 8))\n",
    "    \n",
    "    ss_s = ['G', 'H', 'I', 'B', 'E', 'S', 'T', 'C']\n",
    "    \n",
    "    for index, ss in enumerate(secondary_structures):\n",
    "        if ss == 'G' or ss == 'H' or ss == 'I':\n",
    "            ss3[index, 0], ss8[index, ss_s.index(ss)] = 1, 1\n",
    "        elif ss == 'B' or ss == 'E':\n",
    "            ss3[index, 1], ss8[index, ss_s.index(ss)] = 1, 1\n",
    "        elif ss == 'S' or ss == 'T' or ss == 'C':\n",
    "            ss3[index, 2], ss8[index, ss_s.index(ss)] = 1, 1\n",
    "    \n",
    "    phi = np.zeros(shape=(len(phi_angles), 1))\n",
    "    psi = np.zeros(shape=(len(psi_angles), 1))\n",
    "    \n",
    "    for index, (phi_angle, psi_angle) in enumerate(zip(phi_angles, psi_angles)):\n",
    "        phi[index, 0] = phi_angle if -180 < phi_angle < 180 else -500\n",
    "        psi[index, 0] = psi_angle if -180 < psi_angle < 180 else -500\n",
    "    \n",
    "    with open(output_dir_path + os.sep + protein_name + \"_ss3.npy\", 'wb') as ss3_file:\n",
    "        np.save(file=ss3_file, arr=ss3)\n",
    "    \n",
    "    with open(output_dir_path + os.sep + protein_name + \"_ss8.npy\", 'wb') as ss8_file:\n",
    "        np.save(file=ss8_file, arr=ss8)\n",
    "    \n",
    "    with open(output_dir_path + os.sep + protein_name + \"_phi.npy\", 'wb') as phi_file:\n",
    "        np.save(file=phi_file, arr=phi)\n",
    "    \n",
    "    with open(output_dir_path + os.sep + protein_name + \"_psi.npy\", 'wb') as psi_file:\n",
    "        np.save(file=psi_file, arr=psi)\n",
    "    \n",
    "    return ss3, ss8, phi, psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets Generator Definitions\n",
    "\n",
    "**`generate_spot_1d_single`**\n",
    "- *SPOT-1D-Single Paper* **:** https://doi.org/10.1093/bioinformatics/btab316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_proteins_lists(list_path, fasta_dir_path, dssp_dir_path, dst_path, dataset_name, max_length=700, spot_1d=False):\n",
    "    with open(list_path, 'r') as proteins_list:\n",
    "        dataset_proteins = [name for name in proteins_list.read().split('\\n') if name != '']\n",
    "    \n",
    "    eligibles, non_eligibles, not_founds = {}, {}, []\n",
    "    \n",
    "    for name in dataset_proteins:\n",
    "        fasta_file_path = fasta_dir_path + os.sep + (name + os.sep if spot_1d else '') + name + \".fasta\"\n",
    "        dssp_file_path = dssp_dir_path + os.sep + (name + os.sep if spot_1d else '') + name + \".dssp\"\n",
    "        \n",
    "        if not os.path.isfile(fasta_file_path) or not os.path.isfile(dssp_file_path):\n",
    "            not_founds.append(name)\n",
    "        else:\n",
    "            with open(fasta_file_path, 'r') as fasta_file:\n",
    "                pseq = fasta_file.read().split('\\n')[1]\n",
    "\n",
    "            if len(pseq) > max_length:\n",
    "                non_eligibles[name] = len(pseq)\n",
    "            else:\n",
    "                eligibles[name] = len(pseq)\n",
    "    \n",
    "    dataset_path = dst_path + os.sep + dataset_name\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    \n",
    "    with open(dataset_path + os.sep + dataset_name + f\"_below_{max_length}_proteins.txt\", 'w') as proteins_file:\n",
    "        eligible_proteins = [name + ',' + str(length) for name, length in eligibles.items()]\n",
    "        proteins_file.write('\\n'.join(eligible_proteins))\n",
    "    \n",
    "    with open(dataset_path + os.sep + dataset_name + f\"_above_{max_length}_proteins.txt\", 'w') as proteins_file:\n",
    "        non_eligible_proteins = [name + ',' + str(length) for name, length in non_eligibles.items()]\n",
    "        proteins_file.write('\\n'.join(non_eligible_proteins))\n",
    "    \n",
    "    with open(dataset_path + os.sep + dataset_name + \"_not_found_proteins.txt\", 'w') as proteins_file:\n",
    "        proteins_file.write('\\n'.join(not_founds))\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_protein_features_labels(fasta_dir_path, dssp_dir_path, dataset_path, name, spot_1d=False):\n",
    "    data_path = dataset_path + os.sep + \"Rawdata\" + os.sep + name\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    fasta_file_path = fasta_dir_path + os.sep + (name + os.sep if spot_1d else '') + name + \".fasta\"\n",
    "    dst_path = data_path + os.sep + name\n",
    "    dssp_file_path = dssp_dir_path + os.sep + (name + os.sep if spot_1d else '') + name + \".dssp\"\n",
    "    \n",
    "    shutil.copyfile(src=fasta_file_path, dst=dst_path + \".fasta\")\n",
    "    \n",
    "    with open(dst_path + \".fasta\", 'r') as fasta_file:\n",
    "        pseq = fasta_file.read().split('\\n')[1]\n",
    "    \n",
    "    if spot_1d:\n",
    "        src_path = fasta_dir_path + os.sep + name + os.sep + name\n",
    "        generate_pssm(pssm_file_path=src_path + \".pssm\", pseq=pseq, output_file_path=dst_path + \"_pssm.npy\")\n",
    "        generate_hhm(hhm_file_path=src_path + \".hhm\", pseq=pseq, output_file_path=dst_path + \"_hhm.npy\")\n",
    "    \n",
    "    generate_pcp(pseq=pseq, output_file_path=dst_path + \"_pcp.npy\")\n",
    "    generate_aa(pseq=pseq, output_file_path=dst_path + \"_aa.npy\")\n",
    "    generate_psp(pseq=pseq, output_file_path=dst_path + \"_psp.npy\")\n",
    "    generate_prottrans(pseq=pseq, output_file_path=dst_path + \"_prottrans.npy\")\n",
    "    generate_esm(protein_name=name, pseq=pseq, output_file_path=dst_path + \"_esm.npy\")\n",
    "    generate_ss3_ss8_phi_psi(protein_name=name, dssp_file_path=dssp_file_path, output_dir_path=data_path)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_dataset(list_path, fasta_dir_path, dssp_dir_path, dst_path, ds_name, max_len=700, spot_1d=False):\n",
    "    dataset_path = dst_path + os.sep + ds_name\n",
    "    eligibles_file_path = dataset_path + os.sep + ds_name + f\"_below_{max_len}_proteins.txt\"\n",
    "    non_eligibles_file_path = dataset_path + os.sep + ds_name + f\"_above_{max_len}_proteins.txt\"\n",
    "    \n",
    "    if not os.path.isfile(eligibles_file_path) or not os.path.isfile(non_eligibles_file_path):\n",
    "        generate_proteins_lists(list_path, fasta_dir_path, dssp_dir_path, dst_path, ds_name, max_len, spot_1d)\n",
    "    \n",
    "    temp_proteins_list_path = dataset_path + os.sep + ds_name + \"_temp_proteins.txt\"\n",
    "    raw_data_path = dataset_path + os.sep + \"Rawdata\"\n",
    "    \n",
    "    if os.path.isfile(temp_proteins_list_path):\n",
    "        with open(temp_proteins_list_path, 'r') as proteins_file:\n",
    "            ds_proteins = [name for name in proteins_file.read().split('\\n') if name != '']\n",
    "    else:\n",
    "        if not os.path.exists(raw_data_path):\n",
    "            with open(eligibles_file_path, 'r') as proteins_file:\n",
    "                ds_proteins = [row.split(',')[0] for row in proteins_file.read().split('\\n') if row != '']\n",
    "            \n",
    "            with open(temp_proteins_list_path, 'w') as proteins_file:\n",
    "                proteins_file.write('\\n'.join(ds_proteins))\n",
    "        else:\n",
    "            ds_proteins = []\n",
    "    \n",
    "    if ds_proteins is []:\n",
    "        if os.path.exists(temp_proteins_list_path):\n",
    "            os.remove(temp_proteins_list_path)\n",
    "        return True\n",
    "    \n",
    "    temp_ds_proteins = [protein_name for protein_name in ds_proteins]\n",
    "    \n",
    "    for name in tqdm(iterable=ds_proteins, desc=f\"{ds_name} Generation Progress\", ncols=100, unit=\"protein\"):\n",
    "        generate_protein_features_labels(fasta_dir_path, dssp_dir_path, dataset_path, name, spot_1d)\n",
    "        temp_ds_proteins.remove(name)\n",
    "        \n",
    "        with open(temp_proteins_list_path, 'w') as proteins_file:\n",
    "            proteins_file.write('\\n'.join(temp_ds_proteins))\n",
    "    \n",
    "    if os.path.exists(temp_proteins_list_path):\n",
    "        os.remove(temp_proteins_list_path)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_spot_1d(max_length=700):\n",
    "    ds_map = {\n",
    "        \"Training\": \"training\", \n",
    "        \"Validation\": \"validation\", \n",
    "        \"TEST2016\": \"TEST2016\", \n",
    "        \"TEST2018\": \"TEST2018\"\n",
    "    }\n",
    "    ds_src_path, ds_dst_path = \"../datasets/SPOT-1D/Data\", \"../datasets/SPOT-1D/Features\"\n",
    "    \n",
    "    for key in ds_map:\n",
    "        list_path = ds_src_path + os.sep + \"Accessions\" + os.sep + ds_map[key] + \"-accessions\"\n",
    "        fasta_dir_path = dssp_dir_path = ds_src_path + os.sep + \"Rawdata\" + os.sep + ds_map[key]\n",
    "        generate_dataset(list_path, fasta_dir_path, dssp_dir_path, ds_dst_path, key, max_length, spot_1d=True)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_spot_1d_single(max_length=700):\n",
    "    ds_map = {\n",
    "        \"Training\": \"train\", \n",
    "        \"Validation\": \"val\", \n",
    "        \"TEST2018\": \"TEST2018\", \n",
    "        \"SPOT-2016\": \"SPOT-2016\", \n",
    "        \"SPOT-2016-HQ\": \"SPOT-2016-HQ\", \n",
    "        \"SPOT-2018\": \"SPOT-2018\", \n",
    "        \"SPOT-2018-HQ\": \"SPOT-2018-HQ\", \n",
    "        \"SPOT-2018-Neff1\": \"neff1-2018\", \n",
    "        \"CASP12-FM\": \"casp12\", \n",
    "        \"CASP13-FM\": \"casp13\"\n",
    "    }\n",
    "    ds_src_path, ds_dst_path = \"../datasets/SPOT-1D-Single/Data\", \"../datasets/SPOT-1D-Single/Features\"\n",
    "    \n",
    "    for key in ds_map:\n",
    "        list_path = ds_src_path + os.sep + \"lists\" + os.sep + ds_map[key] + \".txt\"\n",
    "        fasta_dir_path = ds_src_path + os.sep + \"fasta\"\n",
    "        dssp_dir_path = ds_src_path + os.sep + \"dssp\"\n",
    "        generate_dataset(list_path, fasta_dir_path, dssp_dir_path, ds_dst_path, key, max_length, spot_1d=False)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_casp(max_length=700):\n",
    "    casp_ds_map = {\n",
    "        \"CASP12(49)\": \"CASP12(49)\", \n",
    "        \"CASP12(55)\": \"CASP12(55)\", \n",
    "        \"CASP13(31)\": \"CASP13(31)\", \n",
    "        \"CASP13(32)\": \"CASP13(32)\", \n",
    "        \"CASP-FM\": \"CASP-FM\"\n",
    "    }\n",
    "    ds_src_path, ds_dst_path = \"../datasets/CASP/Data\", \"../datasets/CASP/Features\"\n",
    "    \n",
    "    for key in casp_ds_map:\n",
    "        list_path = ds_src_path + os.sep + \"Accessions\" + os.sep + casp_ds_map[key] + \"-accessions\"\n",
    "        fasta_dir_path = dssp_dir_path = ds_src_path + os.sep + \"Rawdata\" + os.sep + casp_ds_map[key]\n",
    "        generate_dataset(list_path, fasta_dir_path, dssp_dir_path, ds_dst_path, key, max_length, spot_1d=True)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_casp12_fm(max_length=700):\n",
    "    ds_map = {\"CASP12-FM\": \"casp12\"}\n",
    "    ds_src_path, ds_dst_path = \"../datasets/SPOT-1D-Single/Data\", \"../datasets/SPOT-1D-Single/Features\"\n",
    "    \n",
    "    for key in ds_map:\n",
    "        list_path = ds_src_path + os.sep + \"lists\" + os.sep + ds_map[key] + \".txt\"\n",
    "        fasta_dir_path, dssp_dir_path = ds_src_path + os.sep + \"fasta\", ds_src_path + os.sep + \"dssp\"\n",
    "        generate_dataset(list_path, fasta_dir_path, dssp_dir_path, ds_dst_path, key, max_length, spot_1d=False)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def generate_spot_1d_lm(max_length=700):\n",
    "    ds_map = {\"SPOT-2018-Neff1\": \"Neff1-2020\", \"CASP14-FM\": \"casp14\"}\n",
    "    ds_src_path, ds_dst_path = \"../datasets/SPOT-1D-LM/Data\", \"../datasets/SPOT-1D-LM/Features\"\n",
    "    \n",
    "    for key in ds_map:\n",
    "        list_path = ds_src_path + os.sep + \"lists\" + os.sep + ds_map[key] + \".txt\"\n",
    "        fasta_dir_path, dssp_dir_path = ds_src_path + os.sep + \"fasta\", ds_src_path + os.sep + \"dssp\"\n",
    "        generate_dataset(list_path, fasta_dir_path, dssp_dir_path, ds_dst_path, key, max_length, spot_1d=False)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets Generation from SPOT-1D and SPOT-1D-Single Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_spot_1d();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_spot_1d_single();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_casp()\n",
    "generate_casp12_fm();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_spot_1d_lm();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Features Generation for SPOT-1D Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot1d_datasets = {\n",
    "    \"Training\": \"training\", \n",
    "    \"Validation\": \"validation\", \n",
    "    \"TEST2016\": \"TEST2016\", \n",
    "    \"TEST2018\": \"TEST2018\"\n",
    "}\n",
    "\n",
    "for dataset in spot1d_datasets:\n",
    "    in_path = \"../datasets/SPOT-1D/Data/Rawdata\" + os.sep + spot1d_datasets[dataset]\n",
    "    dataset_path = \"../datasets/SPOT-1D/Features\" + os.sep + dataset\n",
    "    out_path = dataset_path + os.sep + \"Rawdata\"\n",
    "    \n",
    "    with open(dataset_path + os.sep + dataset + \"_below_700_proteins.txt\", 'r') as accessions_file:\n",
    "        accessions = [content.split(',')[0] for content in accessions_file.read().split('\\n') if content != '']\n",
    "    \n",
    "    for accession in tqdm(iterable=accessions, desc=f\"{dataset} Running...\", ncols=100, unit=\"protein\"):\n",
    "        in_raw_path, out_raw_path = in_path + os.sep + accession, out_path + os.sep + accession\n",
    "        \n",
    "        with open(in_raw_path + os.sep + accession + \".fasta\", 'r') as fasta_file:\n",
    "            pseq = fasta_file.read().split('\\n')[1]\n",
    "        \n",
    "        generate_contact(\n",
    "            contact_file_path=in_raw_path + os.sep + accession + \".spotcon\", \n",
    "            pseq=pseq, \n",
    "            output_file_path=out_raw_path + os.sep + accession + \"_win10.npy\", \n",
    "            window_size=10\n",
    "        )\n",
    "        generate_contact(\n",
    "            contact_file_path=in_raw_path + os.sep + accession + \".spotcon\", \n",
    "            pseq=pseq, \n",
    "            output_file_path=out_raw_path + os.sep + accession + \"_win20.npy\", \n",
    "            window_size=20\n",
    "        )\n",
    "        generate_contact(\n",
    "            contact_file_path=in_raw_path + os.sep + accession + \".spotcon\", \n",
    "            pseq=pseq, \n",
    "            output_file_path=out_raw_path + os.sep + accession + \"_win50.npy\", \n",
    "            window_size=50\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "casp_datasets = {\"CASP12(49)\": \"CASP12(49)\", \"CASP12(55)\": \"CASP12(55)\", \"CASP13(31)\": \"CASP13(31)\", \"CASP13(32)\": \"CASP13(32)\", \"CASP-FM\": \"CASP-FM\"}\n",
    "\n",
    "for dataset in casp_datasets:\n",
    "    in_path = \"../datasets/CASP/Data/Rawdata\" + os.sep + casp_datasets[dataset]\n",
    "    dataset_path = \"../datasets/CASP/Features\" + os.sep + dataset\n",
    "    out_path = dataset_path + os.sep + \"Rawdata\"\n",
    "    \n",
    "    with open(dataset_path + os.sep + dataset + \"_below_700_proteins.txt\", 'r') as accessions_file:\n",
    "        accessions = [content.split(',')[0] for content in accessions_file.read().split('\\n') if content != '']\n",
    "    \n",
    "    for accession in tqdm(iterable=accessions, desc=f\"{dataset} Running...\", ncols=100, unit=\"protein\"):\n",
    "        in_raw_path = in_path + os.sep + accession + os.sep + accession\n",
    "        out_raw_path = out_path + os.sep + accession + os.sep + accession\n",
    "        \n",
    "        with open(in_raw_path + \".fasta\", 'r') as fasta_file:\n",
    "            pseq = fasta_file.read().split('\\n')[1]\n",
    "        \n",
    "        generate_contact(contact_file_path=in_raw_path + \".spotcon\", pseq=pseq, output_file_path=out_raw_path + \"_win10.npy\", window_size=10)\n",
    "        generate_contact(contact_file_path=in_raw_path + \".spotcon\", pseq=pseq, output_file_path=out_raw_path + \"_win20.npy\", window_size=20)\n",
    "        generate_contact(contact_file_path=in_raw_path + \".spotcon\", pseq=pseq, output_file_path=out_raw_path + \"_win50.npy\", window_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Generation Time Measurement for Different Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name, data_ds_name, features_ds_name = \"SPOT-1D\", \"TEST2018\", \"TEST2018\"  # \"CASP\", \"CASP-FM\", \"CASP-FM\"\n",
    "\n",
    "input_features_path = \"../datasets\" + os.sep + dataset_name + os.sep + \"Data/Rawdata\" + os.sep + data_ds_name\n",
    "dataset_path = \"../datasets\" + os.sep + dataset_name + os.sep + \"Features\" + os.sep + features_ds_name\n",
    "output_features_path = \"../temporary\" + os.sep + features_ds_name\n",
    "\n",
    "if os.path.exists(output_features_path):\n",
    "    shutil.rmtree(output_features_path)\n",
    "\n",
    "if not os.path.exists(output_features_path):\n",
    "    os.makedirs(output_features_path)\n",
    "\n",
    "features_generation_start_time = time.time()\n",
    "\n",
    "with open(dataset_path + os.sep + features_ds_name + \"_below_700_proteins.txt\", 'r') as accessions_file:\n",
    "    accessions = [content.split(',')[0] for content in accessions_file.read().split('\\n') if content != '']\n",
    "\n",
    "proteins_dict = dict()\n",
    "\n",
    "for accession in accessions:\n",
    "    if not os.path.exists(output_features_path + os.sep + accession):\n",
    "        os.makedirs(output_features_path + os.sep + accession)\n",
    "    \n",
    "    with open(input_features_path + os.sep + accession + os.sep + accession + \".fasta\", 'r') as fasta_file:\n",
    "        pseq = fasta_file.read().split('\\n')[1]\n",
    "    \n",
    "    proteins_dict[accession] = {\"pseq\": pseq, \"output_file_path\": output_features_path + os.sep + accession + os.sep + accession}\n",
    "\n",
    "for accession in tqdm(iterable=accessions, desc=f\"{features_ds_name} Running...\", ncols=100, unit=\"protein\"):\n",
    "    src_path, dst_path = input_features_path + os.sep + accession + os.sep + accession, proteins_dict[accession][\"output_file_path\"]\n",
    "    \n",
    "    generate_pssm(pssm_file_path=src_path + \".pssm\", pseq=proteins_dict[accession][\"pseq\"], output_file_path=dst_path + \"_pssm.npy\")\n",
    "    generate_hhm(hhm_file_path=src_path + \".hhm\", pseq=proteins_dict[accession][\"pseq\"], output_file_path=dst_path + \"_hhm.npy\")\n",
    "    generate_pcp(pseq=proteins_dict[accession][\"pseq\"], output_file_path=dst_path + \"_pcp.npy\")\n",
    "    generate_contact(\n",
    "        contact_file_path=src_path + \".spotcon\", \n",
    "        pseq=proteins_dict[accession][\"pseq\"], \n",
    "        output_file_path=dst_path + \"_win10.npy\", \n",
    "        window_size=10\n",
    "    )\n",
    "    generate_contact(\n",
    "        contact_file_path=src_path + \".spotcon\", \n",
    "        pseq=proteins_dict[accession][\"pseq\"], \n",
    "        output_file_path=dst_path + \"_win20.npy\", \n",
    "        window_size=20\n",
    "    )\n",
    "    generate_contact(\n",
    "        contact_file_path=src_path + \".spotcon\", \n",
    "        pseq=proteins_dict[accession][\"pseq\"], \n",
    "        output_file_path=dst_path + \"_win50.npy\", \n",
    "        window_size=50\n",
    "    )\n",
    "\n",
    "generate_batch_prottrans(proteins_dict=proteins_dict)\n",
    "\n",
    "features_generation_required_time = time.time() - features_generation_start_time\n",
    "\n",
    "print(f\"Done! ~ Time taken to generate features for SAINT-Evolve is {features_generation_required_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name, data_ds_name, features_ds_name = \"SPOT-1D-Single\", \"TEST2018\", \"TEST2018\"  # \"SPOT-1D-Single\", \"SPOT-2018\", \"SPOT-2018\"\n",
    "\n",
    "input_features_path = \"../datasets\" + os.sep + dataset_name + os.sep + \"Data\" + os.sep + \"fasta\"\n",
    "dataset_path = \"../datasets\" + os.sep + dataset_name + os.sep + \"Features\" + os.sep + features_ds_name\n",
    "output_features_path = \"../temporary\" + os.sep + features_ds_name\n",
    "\n",
    "if os.path.exists(output_features_path):\n",
    "    shutil.rmtree(output_features_path)\n",
    "\n",
    "if not os.path.exists(output_features_path):\n",
    "    os.makedirs(output_features_path)\n",
    "\n",
    "features_generation_start_time = time.time()\n",
    "\n",
    "with open(dataset_path + os.sep + features_ds_name + \"_below_700_proteins.txt\", 'r') as accessions_file:\n",
    "    accessions = [content.split(',')[0] for content in accessions_file.read().split('\\n') if content != '']\n",
    "\n",
    "proteins_dict = dict()\n",
    "\n",
    "for accession in accessions:\n",
    "    if not os.path.exists(output_features_path + os.sep + accession):\n",
    "        os.makedirs(output_features_path + os.sep + accession)\n",
    "    \n",
    "    with open(input_features_path + os.sep + accession + \".fasta\", 'r') as fasta_file:\n",
    "        pseq = fasta_file.read().split('\\n')[1]\n",
    "    \n",
    "    proteins_dict[accession] = {\"pseq\": pseq, \"output_file_path\": output_features_path + os.sep + accession + os.sep + accession}\n",
    "\n",
    "for accession in tqdm(iterable=accessions, desc=f\"{features_ds_name} Running...\", ncols=100, unit=\"protein\"):\n",
    "    generate_aa(pseq=proteins_dict[accession][\"pseq\"], output_file_path=proteins_dict[accession][\"output_file_path\"] + \"_aa.npy\")\n",
    "\n",
    "generate_batch_prottrans(proteins_dict=proteins_dict)\n",
    "generate_batch_esm(proteins_dict=proteins_dict)\n",
    "\n",
    "features_generation_required_time = time.time() - features_generation_start_time\n",
    "\n",
    "print(f\"Done! ~ Time taken to generate features for SAINT-Single is {features_generation_required_time} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
