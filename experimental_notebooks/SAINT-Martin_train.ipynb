{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERe4OEHKwP2F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend, callbacks\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import add, Add, BatchNormalization, Concatenate, Convolution1D, Dense, Dot, Dropout, Embedding, Input, Lambda, Layer, LayerNormalization, Permute, RepeatVector, Softmax, TimeDistributed\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oBqSil1AQdQ"
   },
   "source": [
    "### Prediction Pipeline Architecture Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfo5YBjhEHmk"
   },
   "source": [
    "#### Inception Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFWQUw6JElH8"
   },
   "outputs": [],
   "source": [
    "def inception_block(x, num_inception_filters=100, drop_rate=0.1):\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    conv1_1 = Convolution1D(filters=num_inception_filters, kernel_size=1, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(x)\n",
    "    conv1_1 = Dropout(rate=drop_rate)(conv1_1)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "\n",
    "    conv2_1 = Convolution1D(filters=num_inception_filters, kernel_size=1, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(x)\n",
    "    conv2_1 = Dropout(rate=drop_rate)(conv2_1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    conv2_2 = Convolution1D(filters=num_inception_filters, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(conv2_1)\n",
    "    conv2_2 = Dropout(rate=drop_rate)(conv2_2)\n",
    "    conv2_2 = BatchNormalization()(conv2_2)\n",
    "\n",
    "    conv3_1 = Convolution1D(filters=num_inception_filters, kernel_size=1, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(x)\n",
    "    conv3_1 = Dropout(rate=drop_rate)(conv3_1)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    conv3_2 = Convolution1D(filters=num_inception_filters, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(conv3_1)\n",
    "    conv3_2 = Dropout(rate=drop_rate)(conv3_2)\n",
    "    conv3_2 = BatchNormalization()(conv3_2)\n",
    "    conv3_3 = Convolution1D(filters=num_inception_filters, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(conv3_2)\n",
    "    conv3_3 = Dropout(rate=drop_rate)(conv3_3)\n",
    "    conv3_3 = BatchNormalization()(conv3_3)\n",
    "    conv3_4 = Convolution1D(filters=num_inception_filters, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(conv3_3)\n",
    "    conv3_4 = Dropout(rate=drop_rate)(conv3_4)\n",
    "    conv3_4 = BatchNormalization()(conv3_4)\n",
    "\n",
    "    concat = Concatenate()([conv1_1, conv2_2, conv3_4])\n",
    "    concat = BatchNormalization()(concat)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4AiwLdEtNJ"
   },
   "source": [
    "#### Scaled Dot-product Submodule of Self-attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA9Yq1GXE-wM"
   },
   "outputs": [],
   "source": [
    "def get_shape_list(x):\n",
    "    if backend.backend() != \"theano\":\n",
    "        temp = backend.int_shape(x)\n",
    "    else:\n",
    "        temp = x.shape\n",
    "\n",
    "    temp = list(temp)\n",
    "    temp[0] = -1\n",
    "    return temp\n",
    "\n",
    "def scaled_dot_product(activations, attn_mask):\n",
    "    drop_rate, units = 0.1, int(activations.shape[2])\n",
    "    \n",
    "    Q = TimeDistributed(Dense(units=units, activation=None, use_bias=False))(activations)\n",
    "    Q = Dropout(rate=drop_rate)(Q)\n",
    "    K = TimeDistributed(Dense(units=units, activation=None, use_bias=False))(activations)\n",
    "    K = Dropout(rate=drop_rate)(K)\n",
    "    V = TimeDistributed(Dense(units=units, activation=None, use_bias=False))(activations)\n",
    "    V = Dropout(rate=drop_rate)(V)\n",
    "\n",
    "    QK_T = Dot(axes=-1, normalize=False)([Q, K])\n",
    "    QK_T = Lambda(lambda x: x[0] / backend.sqrt(backend.cast(get_shape_list(x[1])[-1], \"float32\")))([QK_T, V])\n",
    "    \n",
    "    attention_mask = RepeatVector(n=QK_T.shape[1])(attn_mask)\n",
    "    QK_T = Add()([QK_T, attention_mask])\n",
    "    QK_T = Softmax(axis=-1)(QK_T)\n",
    "    QK_T = Dropout(rate=drop_rate)(QK_T)\n",
    "\n",
    "    V = Permute(dims=[2, 1])(V)\n",
    "    V_prime = Dot(axes=-1, normalize=False)([QK_T, V])\n",
    "    return V_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1BX4GytFHaH"
   },
   "source": [
    "#### Self-attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pHZAr-zFQ7q"
   },
   "outputs": [],
   "source": [
    "class CustomLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._x = None\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self._x = backend.variable(0.2)\n",
    "        self._x._trainable = True\n",
    "        self._trainable_weights = [self._x]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        output_after_attention, previous_layer_input = x\n",
    "        result = add([self._x * output_after_attention, (1 - self._x) * previous_layer_input])\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "def get_position_encoding(protein_len: int, d_emb: int) -> np.array:\n",
    "    position_encoding = np.array(object=[[pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] if pos != 0 else np.zeros(d_emb) for pos in range(protein_len)], dtype=np.float32)\n",
    "    position_encoding[1:, 0::2] = np.sin(position_encoding[1:, 0::2])\n",
    "    position_encoding[1:, 1::2] = np.cos(position_encoding[1:, 1::2])\n",
    "    return position_encoding\n",
    "\n",
    "def get_position_embedding(position_ids, output_dim=50):\n",
    "    protein_len, output_dim = 700, int(output_dim)\n",
    "    position_embedding = Dropout(rate=0.1)(Embedding(protein_len, output_dim, trainable=False, input_length=protein_len, weights=[get_position_encoding(protein_len, output_dim)])(position_ids))\n",
    "    position_embedding = LayerNormalization(epsilon=1e-5)(position_embedding)\n",
    "    return position_embedding\n",
    "\n",
    "def self_attention_module(x, attention_mask, position_ids=None, drop_rate=0.1, use_attention=True):\n",
    "    if not use_attention:\n",
    "        return x\n",
    "    \n",
    "    original_dim = int(x.shape[-1])\n",
    "    \n",
    "    if position_ids is not None:\n",
    "        position_embedding = get_position_embedding(position_ids=position_ids, output_dim=original_dim)\n",
    "        x = Add()([x, position_embedding])\n",
    "    \n",
    "    attention_layer = scaled_dot_product(activations=x, attn_mask=attention_mask)\n",
    "    attention_layer = Dropout(rate=drop_rate)(attention_layer)\n",
    "    \n",
    "    x = CustomLayer()([attention_layer, x])\n",
    "    x = Dropout(rate=drop_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oi6F0zAFaMt"
   },
   "source": [
    "#### 2A3I Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-6TlhKQFjyd"
   },
   "outputs": [],
   "source": [
    "def residual_connection(previous_layer_feature, current_layer_feature, num_residual_filters=300, use_skip_connection=False):\n",
    "    if not use_skip_connection:\n",
    "        return current_layer_feature\n",
    "    else:\n",
    "        current_layer_feature = Convolution1D(filters=num_residual_filters, kernel_size=1, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(current_layer_feature)\n",
    "        return Add()([previous_layer_feature, current_layer_feature])\n",
    "\n",
    "def deep_2a3i_module(x, attention_mask, position_ids=None, do_batchnorm=True, num_residual_filters=300, num_inception_filters=100, inception_drop_rate=0.1, use_skip_connection=False, use_attention=True):\n",
    "    block1_1 = inception_block(x, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block1_1 = residual_connection(x, block1_1, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block1_1_attn = self_attention_module(block1_1, attention_mask, position_ids, use_attention=use_attention)\n",
    "    block1_1 = residual_connection(block1_1, block1_1_attn, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    \n",
    "    block2_1 = inception_block(x, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block2_1 = residual_connection(x, block2_1, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block2_2 = inception_block(block2_1, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block2_2 = residual_connection(block2_1, block2_2, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block2_2_attn = self_attention_module(block2_2, attention_mask, position_ids, use_attention=use_attention)\n",
    "    block2_2 = residual_connection(block2_2, block2_2_attn, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    \n",
    "    block3_1 = inception_block(x, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block3_1 = residual_connection(x, block3_1, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block3_2 = inception_block(block3_1, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block3_2 = residual_connection(block3_1, block3_2, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block3_3 = inception_block(block3_2, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block3_3 = residual_connection(block3_2, block3_3, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block3_4 = inception_block(block3_3, num_inception_filters=num_inception_filters, drop_rate=inception_drop_rate)\n",
    "    block3_4 = residual_connection(block3_3, block3_4, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    block3_4_attn = self_attention_module(block3_4, attention_mask, position_ids, use_attention=use_attention)\n",
    "    block3_4 = residual_connection(block3_4, block3_4_attn, num_residual_filters=num_residual_filters, use_skip_connection=use_skip_connection)\n",
    "    \n",
    "    concat = Concatenate()([block1_1, block2_2, block3_4])\n",
    "    \n",
    "    if do_batchnorm:\n",
    "        concat = BatchNormalization()(concat)\n",
    "    \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pG77-1UFpIh"
   },
   "source": [
    "#### Core Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEJdi6ctDqlT"
   },
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    drop_rate = 0.4\n",
    "    input_features = []\n",
    "    start_index = end_index = 0\n",
    "    \n",
    "    main_input = Input(shape=(700, args[\"num_features\"]), name=\"main_input\")\n",
    "    attention_mask = Input(shape=(700,), name=\"attention_mask\")\n",
    "    position_ids = Input(batch_shape=(None, 700), name=\"position_ids\", dtype=\"int32\")\n",
    "    \n",
    "    if args[\"features\"][\"ProtTrans\"][\"add_feature\"]:\n",
    "        end_index = start_index + args[\"features\"][\"ProtTrans\"][\"feature_len\"]\n",
    "        prottrans_features = main_input[:, :, start_index:end_index]\n",
    "        \n",
    "        if args[\"num_prottrans_filters\"] == args[\"features\"][\"ProtTrans\"][\"feature_len\"]:\n",
    "            input_features.append(prottrans_features)\n",
    "        else:\n",
    "            conv_prottrans_features = Convolution1D(filters=args[\"num_prottrans_filters\"], kernel_size=7, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(prottrans_features)\n",
    "            input_features.append(conv_prottrans_features)\n",
    "    if args[\"features\"][\"ESM1b\"][\"add_feature\"]:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + args[\"features\"][\"ESM1b\"][\"feature_len\"]\n",
    "        esm1b_features = main_input[:, :, start_index:end_index]\n",
    "        \n",
    "        if args[\"num_esm1b_filters\"] == args[\"features\"][\"ESM1b\"][\"feature_len\"]:\n",
    "            input_features.append(esm1b_features)\n",
    "        else:\n",
    "            conv_esm1b_features = Convolution1D(filters=args[\"num_esm1b_filters\"], kernel_size=7, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(esm1b_features)\n",
    "            input_features.append(conv_esm1b_features)\n",
    "    if args[\"features\"][\"ProtTransBFD\"][\"add_feature\"]:\n",
    "        start_index = end_index\n",
    "        end_index = start_index + args[\"features\"][\"ProtTransBFD\"][\"feature_len\"]\n",
    "        prottransBFD_features = main_input[:, :, start_index:end_index]\n",
    "        \n",
    "        if args[\"num_prottransBFD_filters\"] == args[\"features\"][\"ProtTransBFD\"][\"feature_len\"]:\n",
    "            input_features.append(prottransBFD_features)\n",
    "        else:\n",
    "            conv_prottransBFD_features = Convolution1D(filters=args[\"num_prottransBFD_filters\"], kernel_size=7, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(prottransBFD_features)\n",
    "            input_features.append(conv_prottransBFD_features)\n",
    "    if end_index < args[\"num_features\"]:\n",
    "        start_index = end_index\n",
    "        biological_features = main_input[:, :, start_index:]\n",
    "        input_features.append(biological_features)\n",
    "    \n",
    "    if len(input_features) > 1:\n",
    "        input_features = tf.keras.layers.concatenate(input_features, axis=-1)\n",
    "    else:\n",
    "        input_features = input_features[0]\n",
    "    \n",
    "    if args[\"architecture\"] == \"Basic1\" or args[\"architecture\"] == \"Basic2\":\n",
    "        block1 = deep_2a3i_module(input_features, attention_mask, position_ids, num_residual_filters=args[\"num_residual_filters\"], num_inception_filters=args[\"num_inception_filters\"], inception_drop_rate=args[\"inception_drop_rate\"], use_skip_connection=args[\"use_skip_connection\"], use_attention=args[\"use_attention\"])\n",
    "        \n",
    "        if args[\"architecture\"] == \"Basic1\":\n",
    "            output_2a3i = block1\n",
    "        elif args[\"architecture\"] == \"Basic2\":\n",
    "            output_2a3i = deep_2a3i_module(block1, attention_mask, position_ids, num_residual_filters=args[\"num_residual_filters\"], num_inception_filters=args[\"num_inception_filters\"], inception_drop_rate=args[\"inception_drop_rate\"], use_skip_connection=args[\"use_skip_connection\"], use_attention=args[\"use_attention\"])\n",
    "        \n",
    "        output_2a3i_attention = self_attention_module(output_2a3i, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        conv11 = Convolution1D(filters=100, kernel_size=11, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(output_2a3i_attention)\n",
    "        conv11_attention = self_attention_module(conv11, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        \n",
    "        Q8_dense1 = TimeDistributed(Dense(units=256, activation=\"relu\"))(conv11_attention)\n",
    "        Q8_dense1 = Dropout(rate=drop_rate)(Q8_dense1)\n",
    "        Q8_dense1_attention = self_attention_module(Q8_dense1, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        \n",
    "        Phi_Psi_dense1 = TimeDistributed(Dense(units=256, activation=\"relu\"))(conv11_attention)\n",
    "        Phi_Psi_dense1 = Dropout(rate=drop_rate)(Phi_Psi_dense1)\n",
    "        Phi_Psi_dense1_attention = self_attention_module(Phi_Psi_dense1, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "    elif args[\"architecture\"] == \"Residual1\" or args[\"architecture\"] == \"Residual2\":\n",
    "        block1 = deep_2a3i_module(input_features, attention_mask, position_ids, num_residual_filters=args[\"num_residual_filters\"], num_inception_filters=args[\"num_inception_filters\"], inception_drop_rate=args[\"inception_drop_rate\"], use_skip_connection=args[\"use_skip_connection\"], use_attention=args[\"use_attention\"])\n",
    "        block1 = residual_connection(input_features, block1, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "        \n",
    "        if args[\"architecture\"] == \"Residual1\":\n",
    "            output_2a3i = block1\n",
    "        elif args[\"architecture\"] == \"Residual2\":\n",
    "            output_2a3i = deep_2a3i_module(block1, attention_mask, position_ids, num_residual_filters=args[\"num_residual_filters\"], num_inception_filters=args[\"num_inception_filters\"], inception_drop_rate=args[\"inception_drop_rate\"], use_skip_connection=args[\"use_skip_connection\"], use_attention=args[\"use_attention\"])\n",
    "            output_2a3i = residual_connection(block1, output_2a3i, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "        \n",
    "        output_2a3i_attention = self_attention_module(output_2a3i, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        output_2a3i_attention = residual_connection(output_2a3i, output_2a3i_attention, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "        \n",
    "        conv11 = Convolution1D(filters=100, kernel_size=11, activation=\"relu\", padding=\"same\", kernel_regularizer=l2(0.001))(output_2a3i_attention)\n",
    "        conv11 = residual_connection(output_2a3i_attention, conv11, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "        \n",
    "        conv11_attention = self_attention_module(conv11, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        conv11_attention = residual_connection(conv11, conv11_attention, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "        \n",
    "        Q8_dense1 = TimeDistributed(Dense(units=args[\"num_residual_filters\"], activation=\"relu\"))(conv11_attention)\n",
    "        Q8_dense1 = Dropout(rate=drop_rate)(Q8_dense1)\n",
    "        \n",
    "        Q8_dense1_attention = self_attention_module(Q8_dense1, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        Q8_dense1_attention = residual_connection(Q8_dense1, Q8_dense1_attention, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "        \n",
    "        Phi_Psi_dense1 = TimeDistributed(Dense(units=args[\"num_residual_filters\"], activation=\"relu\"))(conv11_attention)\n",
    "        Phi_Psi_dense1 = Dropout(rate=drop_rate)(Phi_Psi_dense1)\n",
    "        \n",
    "        Phi_Psi_dense1_attention = self_attention_module(Phi_Psi_dense1, attention_mask, position_ids, use_attention=args[\"use_attention\"])\n",
    "        Phi_Psi_dense1_attention = residual_connection(Phi_Psi_dense1, Phi_Psi_dense1_attention, num_residual_filters=args[\"num_residual_filters\"], use_skip_connection=args[\"use_skip_connection\"])\n",
    "    \n",
    "    Q8_output = TimeDistributed(Dense(units=8, activation=\"softmax\"), name=\"Q8_output\")(Q8_dense1_attention)\n",
    "    Phi_Psi_output = TimeDistributed(Dense(units=4, activation=\"tanh\"), name=\"Phi_Psi_output\")(Phi_Psi_dense1_attention)\n",
    "    \n",
    "    model = Model(inputs=[main_input, attention_mask, position_ids], outputs=[Q8_output, Phi_Psi_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondary Structure 8-state (Q8) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_categorical_cross_entropy(y_true, y_predicted):\n",
    "    mask = backend.sum(y_true, axis=2)\n",
    "\n",
    "    loss = backend.sum(y_true * backend.log(y_predicted + sys.float_info.epsilon), axis=2)\n",
    "    loss = backend.sum(loss * mask, axis=1)\n",
    "\n",
    "    return -1 * backend.sum(loss, axis=0)\n",
    "\n",
    "def average_accuracy(y_true, y_predicted):\n",
    "    mask = backend.sum(y_true, axis=2)\n",
    "\n",
    "    y_true_labels, y_predicted_labels = backend.cast(backend.argmax(y_true, axis=2), \"int8\"), backend.cast(backend.argmax(y_predicted, axis=2), \"int8\")\n",
    "\n",
    "    is_identical = backend.cast(backend.equal(y_true_labels, y_predicted_labels), \"float32\")\n",
    "    num_identicals, protein_lengths = backend.sum(is_identical * mask, axis=1), backend.sum(mask, axis=1)\n",
    "\n",
    "    return backend.mean(num_identicals / protein_lengths, axis=0)\n",
    "\n",
    "def total_accuracy(y_true, y_predicted):\n",
    "    mask = backend.sum(y_true, axis=2)\n",
    "\n",
    "    y_true_labels, y_predicted_labels = backend.cast(backend.argmax(y_true, axis=2), \"int8\"), backend.cast(backend.argmax(y_predicted, axis=2), \"int8\")\n",
    "\n",
    "    is_identical = backend.cast(backend.equal(y_true_labels, y_predicted_labels), \"float32\")\n",
    "    num_identicals, protein_lengths = backend.sum(is_identical * mask, axis=1), backend.sum(mask, axis=1)\n",
    "\n",
    "    return backend.sum(num_identicals, axis=0) / backend.sum(protein_lengths, axis=0)\n",
    "\n",
    "def total_correct_prediction(y_true, y_predicted):\n",
    "    mask = backend.sum(y_true, axis=2)\n",
    "\n",
    "    y_true_labels, y_predicted_labels = backend.cast(backend.argmax(y_true, axis=2), \"int8\"), backend.cast(backend.argmax(y_predicted, axis=2), \"int8\")\n",
    "\n",
    "    is_identical = backend.cast(backend.equal(y_true_labels, y_predicted_labels), \"float32\")\n",
    "    num_identicals = backend.sum(is_identical * mask, axis=1)\n",
    "    \n",
    "    return backend.sum(num_identicals, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone Torsion φ and ψ Angles Prediction\n",
    "\n",
    "**Performance Metrics:-**\n",
    "- `tse` -> *Total Squared Error (TSE)*\n",
    "- `mse` -> *Mean Squared Error (MSE)*\n",
    "- `mae` -> *Mean Absolute Error (MAE)*\n",
    "- `sae` -> *Sum of Absolute Error (SAE)* = *MAE* * `total_residue_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_tse(y_true, y_predicted):\n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    \n",
    "    y_true_phi_sine, y_true_phi_cosine = y_true[:, :, 0] * mask, y_true[:, :, 1] * mask\n",
    "    y_true_psi_sine, y_true_psi_cosine = y_true[:, :, 2] * mask, y_true[:, :, 3] * mask\n",
    "    y_pred_phi_sine, y_pred_phi_cosine = y_predicted[:, :, 0] * mask, y_predicted[:, :, 1] * mask\n",
    "    y_pred_psi_sine, y_pred_psi_cosine = y_predicted[:, :, 2] * mask, y_predicted[:, :, 3] * mask\n",
    "    \n",
    "    phi_diff_sine, phi_diff_cosine = backend.abs(y_true_phi_sine - y_pred_phi_sine), backend.abs(y_true_phi_cosine - y_pred_phi_cosine)\n",
    "    psi_diff_sine, psi_diff_cosine = backend.abs(y_true_psi_sine - y_pred_psi_sine), backend.abs(y_true_psi_cosine - y_pred_psi_cosine)\n",
    "    phi_mse_sine, phi_mse_cosine = backend.sum(backend.square(phi_diff_sine)), backend.sum(backend.square(phi_diff_cosine))\n",
    "    psi_mse_sine, psi_mse_cosine = backend.sum(backend.square(psi_diff_sine)), backend.sum(backend.square(psi_diff_cosine))\n",
    "    \n",
    "    return phi_mse_sine + phi_mse_cosine + psi_mse_sine + psi_mse_cosine\n",
    "\n",
    "def mean_tse(y_true, y_predicted):\n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    \n",
    "    y_true_phi_sine, y_true_phi_cosine = y_true[:, :, 0] * mask, y_true[:, :, 1] * mask\n",
    "    y_true_psi_sine, y_true_psi_cosine = y_true[:, :, 2] * mask, y_true[:, :, 3] * mask\n",
    "    y_pred_phi_sine, y_pred_phi_cosine = y_predicted[:, :, 0] * mask, y_predicted[:, :, 1] * mask\n",
    "    y_pred_psi_sine, y_pred_psi_cosine = y_predicted[:, :, 2] * mask, y_predicted[:, :, 3] * mask\n",
    "    \n",
    "    phi_diff_sine, phi_diff_cosine = backend.abs(y_true_phi_sine - y_pred_phi_sine), backend.abs(y_true_phi_cosine - y_pred_phi_cosine)\n",
    "    psi_diff_sine, psi_diff_cosine = backend.abs(y_true_psi_sine - y_pred_psi_sine), backend.abs(y_true_psi_cosine - y_pred_psi_cosine)\n",
    "    phi_mse_sine, phi_mse_cosine = backend.sum(backend.square(phi_diff_sine)), backend.sum(backend.square(phi_diff_cosine))\n",
    "    psi_mse_sine, psi_mse_cosine = backend.sum(backend.square(psi_diff_sine)), backend.sum(backend.square(psi_diff_cosine))\n",
    "    \n",
    "    return 0.25 * (phi_mse_sine + phi_mse_cosine + psi_mse_sine + psi_mse_cosine)\n",
    "\n",
    "def total_mse(y_true, y_predicted):\n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    total_residue_count = backend.sum(mask)\n",
    "    \n",
    "    y_true_phi_sine, y_true_phi_cosine = y_true[:, :, 0] * mask, y_true[:, :, 1] * mask\n",
    "    y_true_psi_sine, y_true_psi_cosine = y_true[:, :, 2] * mask, y_true[:, :, 3] * mask\n",
    "    y_pred_phi_sine, y_pred_phi_cosine = y_predicted[:, :, 0] * mask, y_predicted[:, :, 1] * mask\n",
    "    y_pred_psi_sine, y_pred_psi_cosine = y_predicted[:, :, 2] * mask, y_predicted[:, :, 3] * mask\n",
    "    \n",
    "    phi_diff_sine, phi_diff_cosine = backend.abs(y_true_phi_sine - y_pred_phi_sine), backend.abs(y_true_phi_cosine - y_pred_phi_cosine)\n",
    "    psi_diff_sine, psi_diff_cosine = backend.abs(y_true_psi_sine - y_pred_psi_sine), backend.abs(y_true_psi_cosine - y_pred_psi_cosine)\n",
    "    phi_mse_sine, phi_mse_cosine = backend.sum(backend.square(phi_diff_sine)) / total_residue_count, backend.sum(backend.square(phi_diff_cosine)) / total_residue_count\n",
    "    psi_mse_sine, psi_mse_cosine = backend.sum(backend.square(psi_diff_sine)) / total_residue_count, backend.sum(backend.square(psi_diff_cosine)) / total_residue_count\n",
    "    \n",
    "    total_mse = phi_mse_sine + phi_mse_cosine + psi_mse_sine + psi_mse_cosine\n",
    "    return total_mse\n",
    "\n",
    "def mean_mse(y_true, y_predicted):\n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    total_residue_count = backend.sum(mask)\n",
    "    \n",
    "    y_true_phi_sine, y_true_phi_cosine = y_true[:, :, 0] * mask, y_true[:, :, 1] * mask\n",
    "    y_true_psi_sine, y_true_psi_cosine = y_true[:, :, 2] * mask, y_true[:, :, 3] * mask\n",
    "    y_pred_phi_sine, y_pred_phi_cosine = y_predicted[:, :, 0] * mask, y_predicted[:, :, 1] * mask\n",
    "    y_pred_psi_sine, y_pred_psi_cosine = y_predicted[:, :, 2] * mask, y_predicted[:, :, 3] * mask\n",
    "    \n",
    "    phi_diff_sine, phi_diff_cosine = backend.abs(y_true_phi_sine - y_pred_phi_sine), backend.abs(y_true_phi_cosine - y_pred_phi_cosine)\n",
    "    psi_diff_sine, psi_diff_cosine = backend.abs(y_true_psi_sine - y_pred_psi_sine), backend.abs(y_true_psi_cosine - y_pred_psi_cosine)\n",
    "    phi_mse_sine, phi_mse_cosine = backend.sum(backend.square(phi_diff_sine)) / total_residue_count, backend.sum(backend.square(phi_diff_cosine)) / total_residue_count\n",
    "    psi_mse_sine, psi_mse_cosine = backend.sum(backend.square(psi_diff_sine)) / total_residue_count, backend.sum(backend.square(psi_diff_cosine)) / total_residue_count\n",
    "    \n",
    "    mean_mse = 0.25 * (phi_mse_sine + phi_mse_cosine + psi_mse_sine + psi_mse_cosine)\n",
    "    return mean_mse\n",
    "\n",
    "def mean_mae(y_true, y_predicted):\n",
    "    y_true_phi_angle = tf.atan2(y_true[:, :, 0], y_true[:, :, 1]) * 180 / np.pi\n",
    "    y_pred_phi_angle = tf.atan2(y_predicted[:, :, 0], y_predicted[:, :, 1]) * 180 / np.pi\n",
    "    y_true_psi_angle = tf.atan2(y_true[:, :, 2], y_true[:, :, 3]) * 180 / np.pi\n",
    "    y_pred_psi_angle = tf.atan2(y_predicted[:, :, 2], y_predicted[:, :, 3]) * 180 / np.pi\n",
    "    \n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    total_residue_count = backend.sum(mask)\n",
    "    \n",
    "    phi_diff, psi_diff = backend.abs(y_true_phi_angle - y_pred_phi_angle), backend.abs(y_true_psi_angle - y_pred_psi_angle)\n",
    "    phi_diff_rev, psi_diff_rev = Lambda(lambda x: 360 - x)(phi_diff), Lambda(lambda x: 360 - x)(psi_diff)\n",
    "    \n",
    "    phi_mask = backend.cast(backend.greater(phi_diff[:, :], 180), dtype=\"float32\")\n",
    "    phi_mask_rev = 1 - phi_mask\n",
    "    psi_mask = backend.cast(backend.greater(psi_diff[:, :], 180), dtype=\"float32\")\n",
    "    psi_mask_rev = 1 - psi_mask\n",
    "    \n",
    "    phi_error, psi_error = phi_diff * phi_mask_rev + phi_diff_rev * phi_mask, psi_diff * psi_mask_rev + psi_diff_rev * psi_mask\n",
    "    phi_mae, psi_mae = backend.sum(phi_error * mask) / total_residue_count, backend.sum(psi_error * mask) / total_residue_count\n",
    "    \n",
    "    mean_mae = 0.5 * (phi_mae + psi_mae)\n",
    "    return mean_mae\n",
    "\n",
    "def phi_mae(y_true, y_predicted):\n",
    "    y_true_phi_angle = tf.atan2(y_true[:, :, 0], y_true[:, :, 1]) * 180 / np.pi\n",
    "    y_pred_phi_angle = tf.atan2(y_predicted[:, :, 0], y_predicted[:, :, 1]) * 180 / np.pi\n",
    "    \n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    total_residue_count = backend.sum(mask)\n",
    "    \n",
    "    phi_diff = backend.abs(y_true_phi_angle - y_pred_phi_angle)\n",
    "    phi_diff_rev = Lambda(lambda x: 360 - x)(phi_diff)\n",
    "    \n",
    "    phi_mask = backend.cast(backend.greater(phi_diff[:, :], 180), dtype=\"float32\")\n",
    "    phi_mask_rev = 1 - phi_mask\n",
    "    \n",
    "    phi_error = phi_diff * phi_mask_rev + phi_diff_rev * phi_mask\n",
    "    phi_mae = backend.sum(phi_error * mask) / total_residue_count\n",
    "    return phi_mae\n",
    "\n",
    "def psi_mae(y_true, y_predicted):\n",
    "    y_true_psi_angle = tf.atan2(y_true[:, :, 2], y_true[:, :, 3]) * 180 / np.pi\n",
    "    y_pred_psi_angle = tf.atan2(y_predicted[:, :, 2], y_predicted[:, :, 3]) * 180 / np.pi\n",
    "    \n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    total_residue_count = backend.sum(mask)\n",
    "    \n",
    "    psi_diff = backend.abs(y_true_psi_angle - y_pred_psi_angle)\n",
    "    psi_diff_rev = Lambda(lambda x: 360 - x)(psi_diff)\n",
    "    \n",
    "    psi_mask = backend.cast(backend.greater(psi_diff[:, :], 180), dtype=\"float32\")\n",
    "    psi_mask_rev = 1 - psi_mask\n",
    "    \n",
    "    psi_error = psi_diff * psi_mask_rev + psi_diff_rev * psi_mask\n",
    "    psi_mae = backend.sum(psi_error * mask) / total_residue_count\n",
    "    return psi_mae\n",
    "\n",
    "def mean_sae(y_true, y_predicted):\n",
    "    y_true_phi_angle = tf.atan2(y_true[:, :, 0], y_true[:, :, 1]) * 180 / np.pi\n",
    "    y_pred_phi_angle = tf.atan2(y_predicted[:, :, 0], y_predicted[:, :, 1]) * 180 / np.pi\n",
    "    y_true_psi_angle = tf.atan2(y_true[:, :, 2], y_true[:, :, 3]) * 180 / np.pi\n",
    "    y_pred_psi_angle = tf.atan2(y_predicted[:, :, 2], y_predicted[:, :, 3]) * 180 / np.pi\n",
    "    \n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    \n",
    "    phi_diff, psi_diff = backend.abs(y_true_phi_angle - y_pred_phi_angle), backend.abs(y_true_psi_angle - y_pred_psi_angle)\n",
    "    phi_diff_rev, psi_diff_rev = Lambda(lambda x: 360 - x)(phi_diff), Lambda(lambda x: 360 - x)(psi_diff)\n",
    "    \n",
    "    phi_mask = backend.cast(backend.greater(phi_diff[:, :], 180), dtype=\"float32\")\n",
    "    phi_mask_rev = 1 - phi_mask\n",
    "    psi_mask = backend.cast(backend.greater(psi_diff[:, :], 180), dtype=\"float32\")\n",
    "    psi_mask_rev = 1 - psi_mask\n",
    "    \n",
    "    phi_error, psi_error = phi_diff * phi_mask_rev + phi_diff_rev * phi_mask, psi_diff * psi_mask_rev + psi_diff_rev * psi_mask\n",
    "    phi_sae, psi_sae = backend.sum(phi_error * mask), backend.sum(psi_error * mask)\n",
    "    \n",
    "    mean_sae = 0.5 * (phi_sae + psi_sae)\n",
    "    return mean_sae\n",
    "\n",
    "def phi_sae(y_true, y_predicted):\n",
    "    y_true_phi_angle = tf.atan2(y_true[:, :, 0], y_true[:, :, 1]) * 180 / np.pi\n",
    "    y_pred_phi_angle = tf.atan2(y_predicted[:, :, 0], y_predicted[:, :, 1]) * 180 / np.pi\n",
    "    \n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    \n",
    "    phi_diff = backend.abs(y_true_phi_angle - y_pred_phi_angle)\n",
    "    phi_diff_rev = Lambda(lambda x: 360 - x)(phi_diff)\n",
    "    \n",
    "    phi_mask = backend.cast(backend.greater(phi_diff[:, :], 180), dtype=\"float32\")\n",
    "    phi_mask_rev = 1 - phi_mask\n",
    "    \n",
    "    phi_error = phi_diff * phi_mask_rev + phi_diff_rev * phi_mask\n",
    "    phi_sae = backend.sum(phi_error * mask)\n",
    "    return phi_sae\n",
    "\n",
    "def psi_sae(y_true, y_predicted):\n",
    "    y_true_psi_angle = tf.atan2(y_true[:, :, 2], y_true[:, :, 3]) * 180 / np.pi\n",
    "    y_pred_psi_angle = tf.atan2(y_predicted[:, :, 2], y_predicted[:, :, 3]) * 180 / np.pi\n",
    "    \n",
    "    mask = 1 - backend.cast(backend.equal(y_true[:, :, 0], -500), dtype=\"float32\")\n",
    "    \n",
    "    psi_diff = backend.abs(y_true_psi_angle - y_pred_psi_angle)\n",
    "    psi_diff_rev = Lambda(lambda x: 360 - x)(psi_diff)\n",
    "    \n",
    "    psi_mask = backend.cast(backend.greater(psi_diff[:, :], 180), dtype=\"float32\")\n",
    "    psi_mask_rev = 1 - psi_mask\n",
    "    \n",
    "    psi_error = psi_diff * psi_mask_rev + psi_diff_rev * psi_mask\n",
    "    psi_sae = backend.sum(psi_error * mask)\n",
    "    return psi_sae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykejKkHVHwSz"
   },
   "source": [
    "### `CustomDataLoader` Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset_path, dataset_name, features, num_features, batch_size, shuffle):\n",
    "        self.dataset_path, self.dataset_name = dataset_path, dataset_name\n",
    "        self.features, self.num_features = features, num_features\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "        self.proteins_dict, self.protein_names = None, None\n",
    "        \n",
    "        with open(dataset_path + os.sep + dataset_name + \"_below_700_proteins.txt\", 'r') as proteins_file:\n",
    "            self.proteins_dict = [content.split(',') for content in proteins_file.read().split('\\n') if content != '']\n",
    "            self.proteins_dict = {contents[0]: int(contents[1]) for contents in self.proteins_dict}\n",
    "            self.protein_names = list(self.proteins_dict.keys())\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.protein_names)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.protein_names) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_protein_names = self.protein_names[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        main_input = np.zeros(shape=(len(batch_protein_names), 700, self.num_features))\n",
    "        attention_mask = np.zeros(shape=(len(batch_protein_names), 700))\n",
    "        position_ids = np.zeros(shape=(len(batch_protein_names), 700))\n",
    "        weight_mask = np.ones(shape=(len(batch_protein_names), 700))\n",
    "        Q8_labels = np.zeros(shape=(len(batch_protein_names), 700, 8))\n",
    "        phi_psi_labels = np.full(shape=(len(batch_protein_names), 700, 4), fill_value=-500, dtype=np.float32)\n",
    "        \n",
    "        for batch_index, protein_name in enumerate(batch_protein_names):\n",
    "            data_path = self.dataset_path + os.sep + \"Rawdata\" + os.sep + protein_name + os.sep + protein_name\n",
    "            protein_features = []\n",
    "            \n",
    "            for feature in self.features:\n",
    "                if self.features[feature][\"add_feature\"]:\n",
    "                    with open(data_path + '_' + self.features[feature][\"extension\"] + \".npy\", 'rb') as feature_file:\n",
    "                        protein_features.append(np.nan_to_num(np.load(file=feature_file), nan=0.0))\n",
    "            \n",
    "            main_input[batch_index, :self.proteins_dict[protein_name]] = np.concatenate(protein_features, axis=-1)\n",
    "            attention_mask[batch_index, self.proteins_dict[protein_name]:] = -np.inf\n",
    "            position_ids[batch_index] = np.arange(700)\n",
    "            weight_mask[batch_index, self.proteins_dict[protein_name]:] = 0\n",
    "            \n",
    "            with open(data_path + \"_ss8.npy\", 'rb') as label_file:\n",
    "                Q8_labels[batch_index, :self.proteins_dict[protein_name]] = np.load(file=label_file)\n",
    "            \n",
    "            with open(data_path + \"_phi.npy\", 'rb') as label_file:\n",
    "                phi_angles = np.load(file=label_file)\n",
    "                phi_angles = np.reshape(phi_angles, newshape=(-1,))\n",
    "            \n",
    "            with open(data_path + \"_psi.npy\", 'rb') as label_file:\n",
    "                psi_angles = np.load(file=label_file)\n",
    "                psi_angles = np.reshape(psi_angles, newshape=(-1,))\n",
    "            \n",
    "            phi_psi_labels[batch_index, :self.proteins_dict[protein_name], 0] = np.sin(phi_angles * np.pi / 180)\n",
    "            phi_psi_labels[batch_index, :self.proteins_dict[protein_name], 1] = np.cos(phi_angles * np.pi / 180)\n",
    "            phi_psi_labels[batch_index, :self.proteins_dict[protein_name], 2] = np.sin(psi_angles * np.pi / 180)\n",
    "            phi_psi_labels[batch_index, :self.proteins_dict[protein_name], 3] = np.cos(psi_angles * np.pi / 180)\n",
    "            \n",
    "            phi_psi_labels[batch_index, np.where(phi_angles == -500), :] = -500 ##\n",
    "            phi_psi_labels[batch_index, np.where(psi_angles == -500), :] = -500 ##\n",
    "            \n",
    "            phi_psi_labels[batch_index, 0, :] = -500 ##\n",
    "            phi_psi_labels[batch_index, self.proteins_dict[protein_name] - 1, :] = -500 ##\n",
    "        \n",
    "        return {\"main_input\": main_input, \"attention_mask\": attention_mask, \"position_ids\": position_ids}, {\"Q8_output\": Q8_labels, \"Phi_Psi_output\": phi_psi_labels}, weight_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CustomStepLR` Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStepLR(callbacks.Callback):\n",
    "    def __init__(self, step_size, factor, min_lr):\n",
    "        super().__init__()\n",
    "        self.step_size, self.current_step, self.factor, self.min_lr = step_size, 0, factor, min_lr\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Training Epoch {epoch + 1}:-\\nAvg Accuracy: {logs['Q8_output_average_accuracy']}\")\n",
    "        print(f\"Total Accuracy: {logs['Q8_output_total_accuracy']}\")\n",
    "        print(f\"Avg Total Correct Prediction: {logs['Q8_output_total_correct_prediction']}\")\n",
    "        print(f\"Mean MAE: {logs['Phi_Psi_output_mean_mae']}\\nPhi MAE: {logs['Phi_Psi_output_phi_mae']}\")\n",
    "        print(f\"Psi MAE: {logs['Phi_Psi_output_psi_mae']}\\n\")\n",
    "        print(f\"Validation Epoch {epoch + 1}:-\\nAvg Accuracy: {logs['val_Q8_output_average_accuracy']}\")\n",
    "        print(f\"Total Accuracy: {logs['val_Q8_output_total_accuracy']}\")\n",
    "        print(f\"Avg Total Correct Prediction: {logs['val_Q8_output_total_correct_prediction']}\")\n",
    "        print(f\"Mean MAE: {logs['val_Phi_Psi_output_mean_mae']}\\nPhi MAE: {logs['val_Phi_Psi_output_phi_mae']}\")\n",
    "        print(f\"Psi MAE: {logs['val_Phi_Psi_output_psi_mae']}\\n\")\n",
    "        \n",
    "        self.current_step = self.current_step + 1\n",
    "        \n",
    "        if self.current_step == self.step_size:\n",
    "            self.current_step = 0\n",
    "            current_lr = float(backend.get_value(self.model.optimizer.lr))\n",
    "            \n",
    "            if current_lr > self.min_lr:\n",
    "                reduced_lr = current_lr * self.factor\n",
    "                backend.set_value(self.model.optimizer.lr, reduced_lr)\n",
    "                \n",
    "                print(f\"Reducing current learning rate to {reduced_lr} from {current_lr}...\")\n",
    "            else:\n",
    "                print(f\"Learning rate reached {self.min_lr}, can not be reduced any further...\")\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}: Learning rate is {float(backend.get_value(self.model.optimizer.lr))}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CustomReduceLROnPlateau` Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReduceLROnPlateau(callbacks.Callback):\n",
    "    def __init__(self, monitor_metric, monitor_mode, monitor_value, patience, factor, min_lr):\n",
    "        super().__init__()\n",
    "        self.monitor_metric, self.monitor_mode = monitor_metric, monitor_mode\n",
    "        self.monitor_value = monitor_value if monitor_value is not None else (np.inf if monitor_mode == \"min\" else (-np.inf if monitor_mode == \"max\" else None))\n",
    "        self.init_patience, self.current_patience, self.factor, self.min_lr = patience, patience, factor, min_lr\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Training Epoch {epoch + 1}:-\\nAvg Accuracy: {logs['Q8_output_average_accuracy']}\")\n",
    "        print(f\"Total Accuracy: {logs['Q8_output_total_accuracy']}\")\n",
    "        print(f\"Avg Total Correct Prediction: {logs['Q8_output_total_correct_prediction']}\")\n",
    "        print(f\"Mean MAE: {logs['Phi_Psi_output_mean_mae']}\\nPhi MAE: {logs['Phi_Psi_output_phi_mae']}\")\n",
    "        print(f\"Psi MAE: {logs['Phi_Psi_output_psi_mae']}\\n\")\n",
    "        print(f\"Validation Epoch {epoch + 1}:-\\nAvg Accuracy: {logs['val_Q8_output_average_accuracy']}\")\n",
    "        print(f\"Total Accuracy: {logs['val_Q8_output_total_accuracy']}\")\n",
    "        print(f\"Avg Total Correct Prediction: {logs['val_Q8_output_total_correct_prediction']}\")\n",
    "        print(f\"Mean MAE: {logs['val_Phi_Psi_output_mean_mae']}\\nPhi MAE: {logs['val_Phi_Psi_output_phi_mae']}\")\n",
    "        print(f\"Psi MAE: {logs['val_Phi_Psi_output_psi_mae']}\\n\")\n",
    "        \n",
    "        assert self.monitor_value is not None\n",
    "        \n",
    "        if self.monitor_mode == \"min\":\n",
    "            if logs[self.monitor_metric] < self.monitor_value:\n",
    "                print(f\"{self.monitor_metric} decreased from {self.monitor_value} to {logs[self.monitor_metric]}...\")\n",
    "                \n",
    "                self.monitor_value = logs[self.monitor_metric]\n",
    "                self.current_patience = self.init_patience\n",
    "            else:\n",
    "                print(f\"{self.monitor_metric} did not decrease from {self.monitor_value}...\")\n",
    "                \n",
    "                if epoch > 0 and self.current_patience > 0:\n",
    "                    self.current_patience = self.current_patience - 1\n",
    "                elif epoch > 0 and self.current_patience == 0:\n",
    "                    current_lr = float(backend.get_value(self.model.optimizer.lr))\n",
    "                    \n",
    "                    if current_lr > self.min_lr:\n",
    "                        reduced_lr = current_lr * self.factor\n",
    "                        backend.set_value(self.model.optimizer.lr, reduced_lr)\n",
    "                        self.current_patience = self.init_patience\n",
    "                        \n",
    "                        print(f\"Reducing current learning rate to {reduced_lr} from {current_lr}...\")\n",
    "                    else:\n",
    "                        print(f\"Learning rate reached {self.min_lr}, can not be reduced any further...\")\n",
    "        elif self.monitor_mode == \"max\":\n",
    "            if logs[self.monitor_metric] > self.monitor_value:\n",
    "                print(f\"{self.monitor_metric} increased from {self.monitor_value} to {logs[self.monitor_metric]}...\")\n",
    "                \n",
    "                self.monitor_value = logs[self.monitor_metric]\n",
    "                self.current_patience = self.init_patience\n",
    "            else:\n",
    "                print(f\"{self.monitor_metric} did not increase from {self.monitor_value}...\")\n",
    "                \n",
    "                if epoch > 0 and self.current_patience > 0:\n",
    "                    self.current_patience = self.current_patience - 1\n",
    "                elif epoch > 0 and self.current_patience == 0:\n",
    "                    current_lr = float(backend.get_value(self.model.optimizer.lr))\n",
    "                    \n",
    "                    if current_lr > self.min_lr:\n",
    "                        reduced_lr = current_lr * self.factor\n",
    "                        backend.set_value(self.model.optimizer.lr, reduced_lr)\n",
    "                        self.current_patience = self.init_patience\n",
    "                        \n",
    "                        print(f\"Reducing current learning rate to {reduced_lr} from {current_lr}...\")\n",
    "                    else:\n",
    "                        print(f\"Learning rate reached {self.min_lr}, can not be reduced any further...\")\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}: Learning rate is {float(backend.get_value(self.model.optimizer.lr))}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LegacyReduceLROnPlateau` Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegacyReduceLROnPlateau(callbacks.Callback):\n",
    "    def __init__(self, monitor_value, patience, factor, min_lr, data_args):\n",
    "        super().__init__()\n",
    "        self.monitor_value = monitor_value if monitor_value is not None else np.inf\n",
    "        self.init_patience, self.current_patience, self.factor, self.min_lr = patience, patience, factor, min_lr\n",
    "        self.data_args = data_args\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_avg_accuracy, train_total_accuracy = logs[\"Q8_output_average_accuracy\"], logs[\"Q8_output_total_accuracy\"]\n",
    "        train_avg_total_correct_prediction = logs[\"Q8_output_total_correct_prediction\"]\n",
    "        \n",
    "        train_legacy_accuracy = train_avg_total_correct_prediction * self.data_args[\"num_train_batches\"] / self.data_args[\"num_train_residues\"]\n",
    "        \n",
    "        val_avg_accuracy, val_total_accuracy = logs[\"val_Q8_output_average_accuracy\"], logs[\"val_Q8_output_total_accuracy\"]\n",
    "        val_avg_total_correct_prediction = logs[\"val_Q8_output_total_correct_prediction\"]\n",
    "        \n",
    "        val_legacy_accuracy = val_avg_total_correct_prediction * self.data_args[\"num_val_batches\"] / self.data_args[\"num_val_residues\"]\n",
    "        \n",
    "        train_mean_mae, train_phi_mae, train_psi_mae = logs[\"Phi_Psi_output_mean_mae\"], logs[\"Phi_Psi_output_phi_mae\"], logs[\"Phi_Psi_output_psi_mae\"]\n",
    "        train_mean_sae, train_phi_sae, train_psi_sae = logs[\"Phi_Psi_output_mean_sae\"], logs[\"Phi_Psi_output_phi_sae\"], logs[\"Phi_Psi_output_psi_sae\"]\n",
    "        \n",
    "        train_legacy_mean_mae = train_mean_sae * self.data_args[\"num_train_batches\"] / (self.data_args[\"num_train_residues\"] - 2 * self.data_args[\"num_train_proteins\"])\n",
    "        train_legacy_phi_mae = train_phi_sae * self.data_args[\"num_train_batches\"] / (self.data_args[\"num_train_residues\"] - 2 * self.data_args[\"num_train_proteins\"])\n",
    "        train_legacy_psi_mae = train_psi_sae * self.data_args[\"num_train_batches\"] / (self.data_args[\"num_train_residues\"] - 2 * self.data_args[\"num_train_proteins\"])\n",
    "        \n",
    "        val_mean_mae, val_phi_mae, val_psi_mae = logs[\"val_Phi_Psi_output_mean_mae\"], logs[\"val_Phi_Psi_output_phi_mae\"], logs[\"val_Phi_Psi_output_psi_mae\"]\n",
    "        val_mean_sae, val_phi_sae, val_psi_sae = logs[\"val_Phi_Psi_output_mean_sae\"], logs[\"val_Phi_Psi_output_phi_sae\"], logs[\"val_Phi_Psi_output_psi_sae\"]\n",
    "        \n",
    "        val_legacy_mean_mae = val_mean_sae * self.data_args[\"num_val_batches\"] / (self.data_args[\"num_val_residues\"] - 2 * self.data_args[\"num_val_proteins\"])\n",
    "        val_legacy_phi_mae = val_phi_sae * self.data_args[\"num_val_batches\"] / (self.data_args[\"num_val_residues\"] - 2 * self.data_args[\"num_val_proteins\"])\n",
    "        val_legacy_psi_mae = val_psi_sae * self.data_args[\"num_val_batches\"] / (self.data_args[\"num_val_residues\"] - 2 * self.data_args[\"num_val_proteins\"])\n",
    "        \n",
    "        print(f\"Training Epoch {epoch + 1}:-\\nAvg Accuracy: {train_avg_accuracy}\\nTotal Accuracy: {train_total_accuracy}\")\n",
    "        print(f\"Avg Total Correct Prediction: {train_avg_total_correct_prediction}\\nLegacy Accuracy: {train_legacy_accuracy}\")\n",
    "        print(f\"Mean MAE: {train_mean_mae}\\nPhi MAE: {train_phi_mae}\\nPsi MAE: {train_psi_mae}\")\n",
    "        print(f\"Legacy Mean MAE: {train_legacy_mean_mae}\\nLegacy Phi MAE: {train_legacy_phi_mae}\\nLegacy Psi MAE: {train_legacy_psi_mae}\\n\")\n",
    "        \n",
    "        print(f\"Validation Epoch {epoch + 1}:-\\nAvg Accuracy: {val_avg_accuracy}\\nTotal Accuracy: {val_total_accuracy}\")\n",
    "        print(f\"Avg Total Correct Prediction: {val_avg_total_correct_prediction}\\nLegacy Accuracy: {val_legacy_accuracy}\")\n",
    "        print(f\"Mean MAE: {val_mean_mae}\\nPhi MAE: {val_phi_mae}\\nPsi MAE: {val_psi_mae}\")\n",
    "        print(f\"Legacy Mean MAE: {val_legacy_mean_mae}\\nLegacy Phi MAE: {val_legacy_phi_mae}\\nLegacy Psi MAE: {val_legacy_psi_mae}\\n\")\n",
    "        \n",
    "        if logs[\"val_Phi_Psi_output_mean_mae\"] < self.monitor_value:\n",
    "            print(f\"val_Phi_Psi_output_mean_mae decreased from {self.monitor_value} to {logs['val_Phi_Psi_output_mean_mae']}...\")\n",
    "            \n",
    "            self.monitor_value = logs[\"val_Phi_Psi_output_mean_mae\"]\n",
    "            self.current_patience = self.init_patience\n",
    "        else:\n",
    "            print(f\"val_Phi_Psi_output_mean_mae did not decrease from {self.monitor_value}...\")\n",
    "            \n",
    "            if epoch > 0 and self.current_patience > 0:\n",
    "                self.current_patience = self.current_patience - 1\n",
    "            elif epoch > 0 and self.current_patience == 0:\n",
    "                current_lr = float(backend.get_value(self.model.optimizer.lr))\n",
    "                \n",
    "                if current_lr > self.min_lr:\n",
    "                    reduced_lr = current_lr * self.factor\n",
    "                    backend.set_value(self.model.optimizer.lr, reduced_lr)\n",
    "                    self.current_patience = self.init_patience\n",
    "                    \n",
    "                    print(f\"Reducing current learning rate to {reduced_lr} from {current_lr}...\")\n",
    "                else:\n",
    "                    print(f\"Learning rate reached {self.min_lr}, can not be reduced any further...\")\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}: Learning rate is {float(backend.get_value(self.model.optimizer.lr))}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ModelBackup` Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBackup(callbacks.Callback):\n",
    "    def __init__(self, monitor_metric, monitor_mode, monitor_value, best_model_path, backup_model_path):\n",
    "        super().__init__()\n",
    "        self.monitor_metric, self.monitor_mode = monitor_metric, monitor_mode\n",
    "        self.monitor_value = monitor_value if monitor_value is not None else (np.inf if monitor_mode == \"min\" else (-np.inf if monitor_mode == \"max\" else None))\n",
    "        self.best_model_path, self.backup_model_path = best_model_path, backup_model_path\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        assert self.monitor_value is not None\n",
    "        \n",
    "        if self.monitor_mode == \"min\":\n",
    "            if logs[self.monitor_metric] < self.monitor_value:\n",
    "                self.monitor_value = logs[self.monitor_metric]\n",
    "                shutil.copyfile(src=self.best_model_path, dst=self.backup_model_path)\n",
    "        elif self.monitor_mode == \"max\":\n",
    "            if logs[self.monitor_metric] > self.monitor_value:\n",
    "                self.monitor_value = logs[self.monitor_metric]\n",
    "                shutil.copyfile(src=self.best_model_path, dst=self.backup_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMMdy94PwP2k"
   },
   "source": [
    "### Training Pipeline Arguments and Hyperparameters Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"architecture\": \"Basic1\", \n",
    "    \n",
    "    \"features\": {\n",
    "        \"ProtTrans\": {\"add_feature\": False, \"feature_len\": 1024, \"extension\": \"prottrans\"}, \n",
    "        \"ESM1b\": {\"add_feature\": False, \"feature_len\": 1280, \"extension\": \"esm\"}, \n",
    "        \"ProtTransBFD\": {\"add_feature\": True, \"feature_len\": 1024, \"extension\": \"bfd\"}, \n",
    "        \"PSSM\": {\"add_feature\": False, \"feature_len\": 20, \"extension\": \"pssm\"}, \n",
    "        \"HHM\": {\"add_feature\": False, \"feature_len\": 30, \"extension\": \"hhm\"}, \n",
    "        \"PCP\": {\"add_feature\": False, \"feature_len\": 7, \"extension\": \"pcp\"}, \n",
    "        \"AA\": {\"add_feature\": False, \"feature_len\": 20, \"extension\": \"aa\"}, \n",
    "        \"PSP\": {\"add_feature\": False, \"feature_len\": 19, \"extension\": \"psp\"}, \n",
    "        \"Win10\": {\"add_feature\": False, \"feature_len\": 16, \"extension\": \"win10\"}, \n",
    "        \"Win20\": {\"add_feature\": False, \"feature_len\": 36, \"extension\": \"win20\"}, \n",
    "        \"Win50\": {\"add_feature\": False, \"feature_len\": 96, \"extension\": \"win50\"}\n",
    "    }, \n",
    "    \"num_features\": None, \n",
    "    \n",
    "    \"num_prottrans_filters\": 1024, \n",
    "    \"num_esm1b_filters\": 1280, \n",
    "    \"num_prottransBFD_filters\": 1024, \n",
    "    \"num_residual_filters\": 300, \n",
    "    \"num_inception_filters\": 100, \n",
    "    \"inception_drop_rate\": None, \n",
    "    \"use_skip_connection\": None, \n",
    "    \"use_attention\": True, \n",
    "    \n",
    "    \"train_batch_size\": 16, \n",
    "    \"validation_batch_size\": 32, \n",
    "    \"train_set_path\": \"../datasets/SPOT-1D-Single/Features/Training\", \n",
    "    \"validation_set_path\": \"../datasets/SPOT-1D-Single/Features/Validation\", \n",
    "    \n",
    "    \"model_name\": \"SAINT_Martin_Single_Basic1_BFD1024\", \n",
    "    \"best_model_path\": None, \n",
    "    \"backup_model_path\": None, \n",
    "    \n",
    "    \"num_epochs\": 100, \n",
    "    \"init_lr\": 1e-3, \n",
    "    \"patience\": 5, \n",
    "    \"factor\": 0.5, \n",
    "    \"min_lr\": 1e-8, \n",
    "    \n",
    "    \"lr_scheduler\": \"LegacyReduceLROnPlateau\", \n",
    "    \"monitor_metric\": \"val_Phi_Psi_output_mean_mae\", \n",
    "    \"monitor_mode\": \"min\", \n",
    "    \n",
    "    \"Q8_loss_weight\": 0.5, \n",
    "    \"Phi_Psi_loss_weight\": 0.5, \n",
    "    \n",
    "    \"do_retrain\": False, \n",
    "    \"retrain_from_lr\": 1e-3, \n",
    "    \"retrain_from_epoch\": 0, \n",
    "    \"retrain_from_monitor_value\": None, \n",
    "    \n",
    "    \"model_args_path\": None\n",
    "}\n",
    "\n",
    "architecture_configs = {\n",
    "    \"Basic1\": {\"inception_drop_rate\": 0.1, \"use_skip_connection\": False}, \n",
    "    \"Basic2\": {\"inception_drop_rate\": 0.1, \"use_skip_connection\": False}, \n",
    "    \"Residual1\": {\"inception_drop_rate\": 0.2, \"use_skip_connection\": True}, \n",
    "    \"Residual2\": {\"inception_drop_rate\": 0.2, \"use_skip_connection\": True}\n",
    "}\n",
    "\n",
    "args[\"num_features\"] = sum([int(args[\"features\"][feature][\"add_feature\"]) * args[\"features\"][feature][\"feature_len\"] for feature in args[\"features\"]])\n",
    "args[\"inception_drop_rate\"] = architecture_configs[args[\"architecture\"]][\"inception_drop_rate\"]\n",
    "args[\"use_skip_connection\"] = architecture_configs[args[\"architecture\"]][\"use_skip_connection\"]\n",
    "args[\"best_model_path\"] = \"../Best_Model\" + os.sep + args[\"model_name\"] + \".h5\"\n",
    "args[\"backup_model_path\"] = \"../Backup_Model\" + os.sep + args[\"model_name\"] + \".h5\"\n",
    "args[\"model_args_path\"] = \"../Best_Model\" + os.sep + args[\"model_name\"] + \"_args.txt\"\n",
    "\n",
    "custom_objects = {\n",
    "    \"CustomLayer\": CustomLayer, \n",
    "    \"backend\": backend, \n",
    "    \"get_shape_list\": get_shape_list, \n",
    "    \"custom_categorical_cross_entropy\": custom_categorical_cross_entropy, \n",
    "    \"average_accuracy\": average_accuracy, \n",
    "    \"total_accuracy\": total_accuracy, \n",
    "    \"total_correct_prediction\": total_correct_prediction, \n",
    "    \"total_tse\": total_tse, \n",
    "    \"mean_tse\": mean_tse, \n",
    "    \"total_mse\": total_mse, \n",
    "    \"mean_mse\": mean_mse, \n",
    "    \"mean_mae\": mean_mae, \n",
    "    \"phi_mae\": phi_mae, \n",
    "    \"psi_mae\": psi_mae, \n",
    "    \"mean_sae\": mean_sae, \n",
    "    \"phi_sae\": phi_sae, \n",
    "    \"psi_sae\": psi_sae\n",
    "}\n",
    "\n",
    "with open(args[\"model_args_path\"], 'w') as args_file:\n",
    "    args_file.write(str(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEkUmKbhWneG"
   },
   "source": [
    "#### Instantiating DataLoaders for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YUslNIYFY3X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = CustomDataLoader(\n",
    "    dataset_path=args[\"train_set_path\"], \n",
    "    dataset_name=\"Training\", \n",
    "    features=args[\"features\"], \n",
    "    num_features=args[\"num_features\"], \n",
    "    batch_size=args[\"train_batch_size\"], \n",
    "    shuffle=True\n",
    ")\n",
    "validation_dataloader = CustomDataLoader(\n",
    "    dataset_path=args[\"validation_set_path\"], \n",
    "    dataset_name=\"Validation\", \n",
    "    features=args[\"features\"], \n",
    "    num_features=args[\"num_features\"], \n",
    "    batch_size=args[\"validation_batch_size\"], \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMItI4Roc9Pd"
   },
   "source": [
    "#### Preparing Model and Setting up Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GYYOHiRwltS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args[\"do_retrain\"]:\n",
    "    model = load_model(args[\"best_model_path\"], custom_objects=custom_objects)\n",
    "    optimizer = Adam(learning_rate=args[\"retrain_from_lr\"])\n",
    "else:\n",
    "    model = get_model(args)\n",
    "    optimizer = Adam(learning_rate=args[\"init_lr\"])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss={\n",
    "        \"Q8_output\": custom_categorical_cross_entropy, \n",
    "        \"Phi_Psi_output\": total_tse\n",
    "    }, \n",
    "    metrics={\n",
    "        \"Q8_output\": [average_accuracy, total_accuracy, total_correct_prediction], \n",
    "        \"Phi_Psi_output\": [mean_mae, phi_mae, psi_mae, mean_sae, phi_sae, psi_sae]\n",
    "    }, \n",
    "    loss_weights={\n",
    "        \"Q8_output\": args[\"Q8_loss_weight\"], \n",
    "        \"Phi_Psi_output\": args[\"Phi_Psi_loss_weight\"]\n",
    "    }, \n",
    "    sample_weight_mode=\"temporal\"\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=args[\"best_model_path\"], \n",
    "    monitor=args[\"monitor_metric\"], \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    mode=args[\"monitor_mode\"]\n",
    ")\n",
    "model_backup = ModelBackup(\n",
    "    monitor_metric=args[\"monitor_metric\"], \n",
    "    monitor_mode=args[\"monitor_mode\"], \n",
    "    monitor_value=None, \n",
    "    best_model_path=args[\"best_model_path\"], \n",
    "    backup_model_path=args[\"backup_model_path\"]\n",
    ")\n",
    "\n",
    "if args[\"lr_scheduler\"] == \"CustomStepLR\":\n",
    "    lr_scheduler = CustomStepLR(step_size=args[\"patience\"], factor=args[\"factor\"], min_lr=args[\"min_lr\"])\n",
    "elif args[\"lr_scheduler\"] == \"CustomReduceLROnPlateau\":\n",
    "    lr_scheduler = CustomReduceLROnPlateau(\n",
    "        monitor_metric=args[\"monitor_metric\"], \n",
    "        monitor_mode=args[\"monitor_mode\"], \n",
    "        monitor_value=None, \n",
    "        patience=args[\"patience\"], \n",
    "        factor=args[\"factor\"], \n",
    "        min_lr=args[\"min_lr\"]\n",
    "    )\n",
    "elif args[\"lr_scheduler\"] == \"LegacyReduceLROnPlateau\":\n",
    "    data_args = {}\n",
    "    \n",
    "    with open(args[\"train_set_path\"] + os.sep + \"Training_below_700_proteins.txt\", 'r') as proteins_file:\n",
    "        data = [datum.split(',') for datum in proteins_file.read().split('\\n') if datum != '']\n",
    "        data_args[\"num_train_batches\"], data_args[\"num_train_proteins\"] = len(train_dataloader), len(data)\n",
    "        data_args[\"num_train_residues\"] = sum([int(datum[1]) for datum in data])\n",
    "        \n",
    "    with open(args[\"validation_set_path\"] + os.sep + \"Validation_below_700_proteins.txt\", 'r') as proteins_file:\n",
    "        data = [datum.split(',') for datum in proteins_file.read().split('\\n') if datum != '']\n",
    "        data_args[\"num_val_batches\"], data_args[\"num_val_proteins\"] = len(validation_dataloader), len(data)\n",
    "        data_args[\"num_val_residues\"] = sum([int(datum[1]) for datum in data])\n",
    "    \n",
    "    lr_scheduler = LegacyReduceLROnPlateau(\n",
    "        monitor_value=None, \n",
    "        patience=args[\"patience\"], \n",
    "        factor=args[\"factor\"], \n",
    "        min_lr=args[\"min_lr\"], \n",
    "        data_args=data_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxt9vqbOdBqY"
   },
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXwbttIwwltS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=train_dataloader, \n",
    "    epochs=args[\"num_epochs\"], \n",
    "    verbose=1, \n",
    "    callbacks=[model_checkpoint, lr_scheduler, model_backup], \n",
    "    validation_data=validation_dataloader, \n",
    "    initial_epoch=(args[\"retrain_from_epoch\"] if args[\"do_retrain\"] else 0), \n",
    "    workers=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tdSXbRgVODOa",
    "1oBqSil1AQdQ",
    "Qfo5YBjhEHmk",
    "F-4AiwLdEtNJ",
    "j1BX4GytFHaH",
    "6oi6F0zAFaMt",
    "7pG77-1UFpIh",
    "Zz6yFl6iGwP2",
    "ykejKkHVHwSz",
    "XMMdy94PwP2k",
    "kEkUmKbhWneG",
    "CgoiM7ZtIAKF",
    "hyK17wwFDVVx"
   ],
   "name": "SAINT_Angle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
